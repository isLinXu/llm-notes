

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>2-轻松分钟玩转书生·浦语大模型趣味Demo &#8212; 大语言模型学习笔记</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'InternLM/2-轻松分钟玩转书生·浦语大模型趣味Demo';</script>
    <link rel="shortcut icon" href="../_static/panda.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3-基于InternLM和Langchain搭建你的知识库" href="3-%E5%9F%BA%E4%BA%8EInternLM%E5%92%8CLangchain%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93.html" />
    <link rel="prev" title="1-书生浦语大模型全链路开源开放体系" href="1-%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">书生·浦语大模型实战营</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1-%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB.html">1-书生浦语大模型全链路开源开放体系</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">2-轻松分钟玩转书生·浦语大模型趣味Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="3-%E5%9F%BA%E4%BA%8EInternLM%E5%92%8CLangchain%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93.html">3-基于InternLM和Langchain搭建你的知识库</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-XTuner%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%95%E5%8D%A1%E4%BD%8E%E6%88%90%E6%9C%AC%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98.html">4-XTuner大模型单卡低成本微调实战</a></li>
<li class="toctree-l2"><a class="reference internal" href="5-LMDeploy%E5%A4%A7%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96%E9%83%A8%E7%BD%B2%E5%AE%9E%E8%B7%B5.html">5-LMDeploy大模型量化部署实践</a></li>

<li class="toctree-l2"><a class="reference internal" href="6-OpenCompass%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97.html">6-OpenCompass大模型评测解读及实战指南</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes/edit/main/InternLM/2-轻松分钟玩转书生·浦语大模型趣味Demo.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes/issues/new?title=Issue%20on%20page%20%2FInternLM/2-轻松分钟玩转书生·浦语大模型趣味Demo.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/InternLM/2-轻松分钟玩转书生·浦语大模型趣味Demo.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>2-轻松分钟玩转书生·浦语大模型趣味Demo</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#internlm">1 大模型及 InternLM 模型简介</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1 什么是大模型？</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.2 InternLM 模型全链条开源</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#internlm-chat-7b-demo">2 InternLM-Chat-7B 智能对话 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.3 代码准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.4 终端运行</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#web-demo">2.5 web demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lagent-demo">3 Lagent 智能体工具调用 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagent">3.3 Lagent 安装</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.4 修改代码</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.5 Demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">4. 浦语·灵笔图文理解创作 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">4.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">4.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">4.3 代码准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">4.4 Demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">5. 通用环境配置</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipconda">5.1 pip、conda 换源</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pip">5.1.1 pip 换源</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conda">5.1.2 conda 换源</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">5.2 配置本地端口</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">5.3 模型下载</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face">5.3.1 Hugging Face</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelscope">5.3.2 ModelScope</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openxlab">5.3.3 OpenXLab</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">6. 课后作业</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="demo">
<h1>2-轻松分钟玩转书生·浦语大模型趣味Demo<a class="headerlink" href="#demo" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<p><img alt="head" src="https://github.com/isLinXu/llm-notes/assets/59380685/8e2ad263-ed2f-4733-a9bc-88bd39476e89" /></p>
<ul class="simple">
<li><p><span class="xref myst">轻松玩转书生·浦语大模型趣味 Demo</span></p>
<ul>
<li><p><span class="xref myst">1 大模型及 InternLM 模型简介</span></p>
<ul>
<li><p><span class="xref myst">1.1 什么是大模型？</span></p></li>
<li><p><span class="xref myst">1.2 InternLM 模型全链条开源</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">2 InternLM-Chat-7B 智能对话 Demo</span></p>
<ul>
<li><p><span class="xref myst">2.1 环境准备</span></p></li>
<li><p><span class="xref myst">2.2 模型下载</span></p></li>
<li><p><span class="xref myst">2.3 代码准备</span></p></li>
<li><p><span class="xref myst">2.4 终端运行</span></p></li>
<li><p><span class="xref myst">2.5 web demo 运行</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">3 Lagent 智能体工具调用 Demo</span></p>
<ul>
<li><p><span class="xref myst">3.1 环境准备</span></p></li>
<li><p><span class="xref myst">3.2 模型下载</span></p></li>
<li><p><span class="xref myst">3.3 Lagent 安装</span></p></li>
<li><p><span class="xref myst">3.4 修改代码</span></p></li>
<li><p><span class="xref myst">3.5 Demo 运行</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">4. 浦语·灵笔图文理解创作 Demo</span></p>
<ul>
<li><p><span class="xref myst">4.1 环境准备</span></p></li>
<li><p><span class="xref myst">4.2 模型下载</span></p></li>
<li><p><span class="xref myst">4.3 代码准备</span></p></li>
<li><p><span class="xref myst">4.4 Demo 运行</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">5. 通用环境配置</span></p>
<ul>
<li><p><span class="xref myst">5.1 pip、conda 换源</span></p>
<ul>
<li><p><span class="xref myst">5.1.1 pip 换源</span></p></li>
<li><p><span class="xref myst">5.1.2 conda 换源</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">5.2 配置本地端口</span></p></li>
<li><p><span class="xref myst">5.3 模型下载</span></p>
<ul>
<li><p><span class="xref myst">5.3.1 Hugging Face</span></p></li>
<li><p><span class="xref myst">5.3.2 ModelScope</span></p></li>
<li><p><span class="xref myst">5.3.3 OpenXLab</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><span class="xref myst">6. 课后作业</span></p></li>
</ul>
</li>
</ul>
<section id="internlm">
<h2>1 大模型及 InternLM 模型简介<a class="headerlink" href="#internlm" title="Permalink to this heading">#</a></h2>
<section id="id1">
<h3>1.1 什么是大模型？<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h3>
<p>  大模型通常指的是机器学习或人工智能领域中参数数量巨大、拥有庞大计算能力和参数规模的模型。这些模型利用大量数据进行训练，并且拥有数十亿甚至数千亿个参数。大模型的出现和发展得益于增长的数据量、计算能力的提升以及算法优化等因素。这些模型在各种任务中展现出惊人的性能，比如自然语言处理、计算机视觉、语音识别等。这种模型通常采用深度神经网络结构，如 <code class="docutils literal notranslate"><span class="pre">Transformer</span></code>、<code class="docutils literal notranslate"><span class="pre">BERT</span></code>、<code class="docutils literal notranslate"><span class="pre">GPT</span></code>（ Generative Pre-trained Transformer ）等。</p>
<p>  大模型的优势在于其能够捕捉和理解数据中更为复杂、抽象的特征和关系。通过大规模参数的学习，它们可以提高在各种任务上的泛化能力，并在未经过大量特定领域数据训练的情况下实现较好的表现。然而，大模型也面临着一些挑战，比如巨大的计算资源需求、高昂的训练成本、对大规模数据的依赖以及模型的可解释性等问题。因此，大模型的应用和发展也需要在性能、成本和道德等多个方面进行权衡和考量。</p>
</section>
<section id="id2">
<h3>1.2 InternLM 模型全链条开源<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h3>
<p>  <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 是一个开源的轻量级训练框架，旨在支持大模型训练而无需大量的依赖。通过单一的代码库，它支持在拥有数千个 <code class="docutils literal notranslate"><span class="pre">GPU</span></code> 的大型集群上进行预训练，并在单个 <code class="docutils literal notranslate"><span class="pre">GPU</span></code> 上进行微调，同时实现了卓越的性能优化。在 <code class="docutils literal notranslate"><span class="pre">1024</span></code> 个 <code class="docutils literal notranslate"><span class="pre">GPU</span></code> 上训练时，<code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 可以实现近 <code class="docutils literal notranslate"><span class="pre">90%</span></code> 的加速效率。</p>
<p>  基于 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 训练框架，上海人工智能实验室已经发布了两个开源的预训练模型：<code class="docutils literal notranslate"><span class="pre">InternLM-7B</span></code> 和 <code class="docutils literal notranslate"><span class="pre">InternLM-20B</span></code>。</p>
<p>  <code class="docutils literal notranslate"><span class="pre">Lagent</span></code> 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 <code class="docutils literal notranslate"><span class="pre">Lagent</span></code> 框架可以更好的发挥 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 的全部性能。</p>
<p><img alt="Lagent" src="https://github.com/isLinXu/llm-notes/assets/59380685/c770c295-02a2-4631-9364-c6b112133c3d" /></p>
<p>  浦语·灵笔是基于书生·浦语大语言模型研发的视觉-语言大模型，提供出色的图文理解和创作能力，结合了视觉和语言的先进技术，能够实现图像到文本、文本到图像的双向转换。使用浦语·灵笔大模型可以轻松的创作一篇图文推文，也能够轻松识别一张图片中的物体，并生成对应的文本描述。</p>
<p>  上述提到的所有模型，都会带领大家一起体验哦！欢迎大家来给 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code>: https://github.com/InternLM/InternLM/ 点点 star 哦！</p>
</section>
</section>
<section id="internlm-chat-7b-demo">
<h2>2 InternLM-Chat-7B 智能对话 Demo<a class="headerlink" href="#internlm-chat-7b-demo" title="Permalink to this heading">#</a></h2>
<p>本小节我们将使用 <a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 中的 A100(1/4) 机器和 <code class="docutils literal notranslate"><span class="pre">InternLM-Chat-7B</span></code> 模型部署一个智能对话 Demo。</p>
<section id="id3">
<h3>2.1 环境准备<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h3>
<p>在 <a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 平台中选择 A100(1/4) 的配置，如下图所示镜像选择 <code class="docutils literal notranslate"><span class="pre">Cuda11.7-conda</span></code>，如下图所示：</p>
<p><img alt="image" src="https://github.com/isLinXu/llm-notes/assets/59380685/66db4166-c2b4-4788-9436-2a5760e9c7c4" /></p>
<p>接下来打开刚刚租用服务器的<code class="docutils literal notranslate"><span class="pre">进入开发机</span></code>，并且打开其中的终端开始环境配置、模型下载和运行 <code class="docutils literal notranslate"><span class="pre">demo</span></code>。</p>
<p><img alt="image-1" src="https://github.com/isLinXu/llm-notes/assets/59380685/4ae11caf-c447-490b-83dc-a28ca8b45aaf" /></p>
<p>进入开发机后，在页面的左上角可以切换 <code class="docutils literal notranslate"><span class="pre">JupyterLab</span></code>、<code class="docutils literal notranslate"><span class="pre">终端</span></code>和 <code class="docutils literal notranslate"><span class="pre">VScode</span></code>，并在终端输入 <code class="docutils literal notranslate"><span class="pre">bash</span></code> 命令，进入 <code class="docutils literal notranslate"><span class="pre">conda</span></code> 环境。如下图所示：</p>
<p><img alt="image-11" src="https://github.com/isLinXu/llm-notes/assets/59380685/c3fc9a44-a0ab-48c8-a1d4-e3e35f226ac6" /></p>
<p>进入 <code class="docutils literal notranslate"><span class="pre">conda</span></code> 环境之后，使用以下命令从本地克隆一个已有的 <code class="docutils literal notranslate"><span class="pre">pytorch</span> <span class="pre">2.0.1</span></code> 的环境</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span><span class="c1"># 请每次使用 jupyter lab 打开终端时务必先执行 bash 命令进入 bash 中</span>
/root/share/install_conda_env_internlm_base.sh<span class="w"> </span>internlm-demo
</pre></div>
</div>
<p>然后使用以下命令激活环境</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>activate<span class="w"> </span>internlm-demo
</pre></div>
</div>
<p>并在环境中安装运行 demo 所需要的依赖。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 升级pip</span>
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip

pip<span class="w"> </span>install<span class="w"> </span><span class="nv">modelscope</span><span class="o">==</span><span class="m">1</span>.9.5
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.35.2
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">streamlit</span><span class="o">==</span><span class="m">1</span>.24.0
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">sentencepiece</span><span class="o">==</span><span class="m">0</span>.1.99
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">accelerate</span><span class="o">==</span><span class="m">0</span>.24.1
</pre></div>
</div>
</section>
<section id="id4">
<h3>2.2 模型下载<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 平台的 <code class="docutils literal notranslate"><span class="pre">share</span></code> 目录下已经为我们准备了全系列的 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
cp<span class="w"> </span>-r<span class="w"> </span>/root/share/temp/model_repos/internlm-chat-7b<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
</pre></div>
</div>
<blockquote>
<div><p>-r 选项表示递归地复制目录及其内容</p>
</div></blockquote>
<p>也可以使用 <code class="docutils literal notranslate"><span class="pre">modelscope</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">snapshot_download</span></code> 函数下载模型，第一个参数为模型名称，参数 <code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> 为模型的下载路径。</p>
<p>在 <code class="docutils literal notranslate"><span class="pre">/root</span></code> 路径下新建目录 <code class="docutils literal notranslate"><span class="pre">model</span></code>，在目录下新建 <code class="docutils literal notranslate"><span class="pre">download.py</span></code> 文件并在其中输入以下内容，粘贴代码后记得保存文件，如下图所示。并运行 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">/root/model/download.py</span></code> 执行下载，模型大小为 14 GB，下载模型大概需要 10~20 分钟</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">modelscope</span> <span class="kn">import</span> <span class="n">snapshot_download</span><span class="p">,</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="s1">&#39;Shanghai_AI_Laboratory/internlm-chat-7b&#39;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s1">&#39;/root/model&#39;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;v1.0.3&#39;</span><span class="p">)</span>
</pre></div>
</div>
<blockquote>
<div><p>注意：使用 <code class="docutils literal notranslate"><span class="pre">pwd</span></code> 命令可以查看当前的路径，<code class="docutils literal notranslate"><span class="pre">JupyterLab</span></code> 左侧目录栏显示为 <code class="docutils literal notranslate"><span class="pre">/root/</span></code> 下的路径。</p>
</div></blockquote>
<p><img alt="image-2" src="https://github.com/isLinXu/llm-notes/assets/59380685/fffe4bb1-bab4-493b-a0c0-2dcf44b67811" /></p>
</section>
<section id="id5">
<h3>2.3 代码准备<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h3>
<p>首先 <code class="docutils literal notranslate"><span class="pre">clone</span></code> 代码，在 <code class="docutils literal notranslate"><span class="pre">/root</span></code> 路径下新建 <code class="docutils literal notranslate"><span class="pre">code</span></code> 目录，然后切换路径, clone 代码.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/root/code
git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/internlm/InternLM.git
</pre></div>
</div>
<p>切换 commit 版本，与教程 commit 版本保持一致，可以让大家更好的复现。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>InternLM
git<span class="w"> </span>checkout<span class="w"> </span>3028f07cb79e5b1d7342f4ad8d11efad3fd13d17
</pre></div>
</div>
<p>将 <code class="docutils literal notranslate"><span class="pre">/root/code/InternLM/web_demo.py</span></code> 中 29 行和 33 行的模型更换为本地的 <code class="docutils literal notranslate"><span class="pre">/root/model/Shanghai_AI_Laboratory/internlm-chat-7b</span></code>。</p>
<p><img alt="image-3" src="https://github.com/isLinXu/llm-notes/assets/59380685/2c1b4358-ef5e-4e71-906c-1f6b45717c0a" /></p>
</section>
<section id="id6">
<h3>2.4 终端运行<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h3>
<p>我们可以在 <code class="docutils literal notranslate"><span class="pre">/root/code/InternLM</span></code> 目录下新建一个 <code class="docutils literal notranslate"><span class="pre">cli_demo.py</span></code> 文件，将以下代码填入其中：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForCausalLM</span>


<span class="n">model_name_or_path</span> <span class="o">=</span> <span class="s2">&quot;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&quot;</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">torch_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">system_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;You are an AI assistant whose name is InternLM (书生·浦语).</span>
<span class="s2">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span>
<span class="s2">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[(</span><span class="n">system_prompt</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;=============Welcome to InternLM chatbot, type &#39;exit&#39; to exit.=============&quot;</span><span class="p">)</span>

<span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s2">&quot;User  &gt;&gt;&gt; &quot;</span><span class="p">)</span>
    <span class="n">input_text</span> <span class="o">=</span> <span class="n">input_text</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">input_text</span> <span class="o">==</span> <span class="s2">&quot;exit&quot;</span><span class="p">:</span>
        <span class="k">break</span>
    <span class="n">response</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="n">messages</span><span class="p">)</span>
    <span class="n">messages</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">input_text</span><span class="p">,</span> <span class="n">response</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;robot &gt;&gt;&gt; </span><span class="si">{</span><span class="n">response</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>然后在终端运行以下命令，即可体验 <code class="docutils literal notranslate"><span class="pre">InternLM-Chat-7B</span></code> 模型的对话能力。对话效果如下所示：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>/root/code/InternLM/cli_demo.py
</pre></div>
</div>
<p><img alt="image-18" src="https://github.com/isLinXu/llm-notes/assets/59380685/3902b57d-bd32-4c04-9afb-38be68614e30" /></p>
</section>
<section id="web-demo">
<h3>2.5 web demo 运行<a class="headerlink" href="#web-demo" title="Permalink to this heading">#</a></h3>
<p>我们切换到 <code class="docutils literal notranslate"><span class="pre">VScode</span></code> 中，运行 <code class="docutils literal notranslate"><span class="pre">/root/code/InternLM</span></code> 目录下的 <code class="docutils literal notranslate"><span class="pre">web_demo.py</span></code> 文件，输入以下命令后，<span class="xref myst"><strong>查看本教程5.2配置本地端口后</strong></span>，将端口映射到本地。在本地浏览器输入 <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:6006</span></code> 即可。</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash
conda<span class="w"> </span>activate<span class="w"> </span>internlm-demo<span class="w">  </span><span class="c1"># 首次进入 vscode 会默认是 base 环境，所以首先切换环境</span>
<span class="nb">cd</span><span class="w"> </span>/root/code/InternLM
streamlit<span class="w"> </span>run<span class="w"> </span>web_demo.py<span class="w"> </span>--server.address<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span>--server.port<span class="w"> </span><span class="m">6006</span>
</pre></div>
</div>
<p><img alt="image-12" src="https://github.com/isLinXu/llm-notes/assets/59380685/2e49a4c3-921e-4509-a7a2-a618366f1364" /></p>
<p>注意：要在浏览器打开 <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:6006</span></code> 页面后，模型才会加载，如下图所示：</p>
<p><img alt="image-5" src="https://github.com/isLinXu/llm-notes/assets/59380685/fbd075d8-7dc0-4c3d-98a7-d2560bba489b" /></p>
<p>在加载完模型之后，就可以与 InternLM-Chat-7B 进行对话了，如下图所示：</p>
<p><img alt="image-6" src="https://github.com/isLinXu/llm-notes/assets/59380685/fa295ff0-1a4a-4d9a-9aab-86dfe8911b59" /></p>
</section>
</section>
<section id="lagent-demo">
<h2>3 Lagent 智能体工具调用 Demo<a class="headerlink" href="#lagent-demo" title="Permalink to this heading">#</a></h2>
<p>本小节我们将使用 <a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 中的 A100(1/4) 机器、<code class="docutils literal notranslate"><span class="pre">InternLM-Chat-7B</span></code> 模型和 <code class="docutils literal notranslate"><span class="pre">Lagent</span></code> 框架部署一个智能工具调用 Demo。</p>
<p>Lagent 是一个轻量级、开源的基于大语言模型的智能体（agent）框架，支持用户快速地将一个大语言模型转变为多种类型的智能体，并提供了一些典型工具为大语言模型赋能。通过 Lagent 框架可以更好的发挥 InternLM 的全部性能。</p>
<p>下面我们就开始动手实现！</p>
<section id="id7">
<h3>3.1 环境准备<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h3>
<p>选择和第一个 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 一样的镜像环境，运行以下命令安装依赖，如果上一个 <code class="docutils literal notranslate"><span class="pre">InternLM-Chat-7B</span></code> 已经配置好环境不需要重复安装.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># 升级pip</span>
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip

pip<span class="w"> </span>install<span class="w"> </span><span class="nv">modelscope</span><span class="o">==</span><span class="m">1</span>.9.5
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.35.2
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">streamlit</span><span class="o">==</span><span class="m">1</span>.24.0
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">sentencepiece</span><span class="o">==</span><span class="m">0</span>.1.99
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">accelerate</span><span class="o">==</span><span class="m">0</span>.24.1
</pre></div>
</div>
</section>
<section id="id8">
<h3>3.2 模型下载<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 平台的 <code class="docutils literal notranslate"><span class="pre">share</span></code> 目录下已经为我们准备了全系列的 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
cp<span class="w"> </span>-r<span class="w"> </span>/root/share/temp/model_repos/internlm-chat-7b<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
</pre></div>
</div>
<blockquote>
<div><p>-r 选项表示递归地复制目录及其内容</p>
</div></blockquote>
<p>也可以在 <code class="docutils literal notranslate"><span class="pre">/root/model</span></code> 路径下新建 <code class="docutils literal notranslate"><span class="pre">download.py</span></code> 文件并在其中输入以下内容，并运行 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">/root/model/download.py</span></code> 执行下载，模型大小为 14 GB，下载模型大概需要 10~20 分钟</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">modelscope</span> <span class="kn">import</span> <span class="n">snapshot_download</span><span class="p">,</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="s1">&#39;Shanghai_AI_Laboratory/internlm-chat-7b&#39;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s1">&#39;/root/model&#39;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;v1.0.3&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="lagent">
<h3>3.3 Lagent 安装<a class="headerlink" href="#lagent" title="Permalink to this heading">#</a></h3>
<p>首先切换路径到 <code class="docutils literal notranslate"><span class="pre">/root/code</span></code> 克隆 <code class="docutils literal notranslate"><span class="pre">lagent</span></code> 仓库，并通过 <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-e</span> <span class="pre">.</span></code> 源码安装 <code class="docutils literal notranslate"><span class="pre">Lagent</span></code></p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/root/code
git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/internlm/lagent.git
<span class="nb">cd</span><span class="w"> </span>/root/code/lagent
git<span class="w"> </span>checkout<span class="w"> </span>511b03889010c4811b1701abb153e02b8e94fb5e<span class="w"> </span><span class="c1"># 尽量保证和教程commit版本一致</span>
pip<span class="w"> </span>install<span class="w"> </span>-e<span class="w"> </span>.<span class="w"> </span><span class="c1"># 源码安装</span>
</pre></div>
</div>
</section>
<section id="id9">
<h3>3.4 修改代码<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<p>由于代码修改的地方比较多，大家直接将 <code class="docutils literal notranslate"><span class="pre">/root/code/lagent/examples/react_web_demo.py</span></code> 内容替换为以下代码</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="kn">import</span> <span class="nn">streamlit</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">from</span> <span class="nn">streamlit.logger</span> <span class="kn">import</span> <span class="n">get_logger</span>

<span class="kn">from</span> <span class="nn">lagent.actions</span> <span class="kn">import</span> <span class="n">ActionExecutor</span><span class="p">,</span> <span class="n">GoogleSearch</span><span class="p">,</span> <span class="n">PythonInterpreter</span>
<span class="kn">from</span> <span class="nn">lagent.agents.react</span> <span class="kn">import</span> <span class="n">ReAct</span>
<span class="kn">from</span> <span class="nn">lagent.llms</span> <span class="kn">import</span> <span class="n">GPTAPI</span>
<span class="kn">from</span> <span class="nn">lagent.llms.huggingface</span> <span class="kn">import</span> <span class="n">HFTransformerCasualLM</span>


<span class="k">class</span> <span class="nc">SessionState</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">init_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize session state variables.&quot;&quot;&quot;</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;assistant&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="c1">#action_list = [PythonInterpreter(), GoogleSearch()]</span>
        <span class="n">action_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">PythonInterpreter</span><span class="p">()]</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;plugin_map&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="n">action</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">action</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">action_list</span>
        <span class="p">}</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_selected&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;plugin_actions&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">clear_state</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Clear the existing session state.&quot;&quot;&quot;</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;assistant&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_selected&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="s1">&#39;chatbot&#39;</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">:</span>
            <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;chatbot&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">_session_history</span> <span class="o">=</span> <span class="p">[]</span>


<span class="k">class</span> <span class="nc">StreamlitUI</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">session_state</span><span class="p">:</span> <span class="n">SessionState</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_streamlit</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">session_state</span> <span class="o">=</span> <span class="n">session_state</span>

    <span class="k">def</span> <span class="nf">init_streamlit</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Streamlit&#39;s UI settings.&quot;&quot;&quot;</span>
        <span class="n">st</span><span class="o">.</span><span class="n">set_page_config</span><span class="p">(</span>
            <span class="n">layout</span><span class="o">=</span><span class="s1">&#39;wide&#39;</span><span class="p">,</span>
            <span class="n">page_title</span><span class="o">=</span><span class="s1">&#39;lagent-web&#39;</span><span class="p">,</span>
            <span class="n">page_icon</span><span class="o">=</span><span class="s1">&#39;./docs/imgs/lagent_icon.png&#39;</span><span class="p">)</span>
        <span class="c1"># st.header(&#39;:robot_face: :blue[Lagent] Web Demo &#39;, divider=&#39;rainbow&#39;)</span>
        <span class="n">st</span><span class="o">.</span><span class="n">sidebar</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;模型控制&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">setup_sidebar</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Setup the sidebar for model and plugin selection.&quot;&quot;&quot;</span>
        <span class="n">model_name</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">sidebar</span><span class="o">.</span><span class="n">selectbox</span><span class="p">(</span>
            <span class="s1">&#39;模型选择：&#39;</span><span class="p">,</span> <span class="n">options</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;gpt-3.5-turbo&#39;</span><span class="p">,</span><span class="s1">&#39;internlm&#39;</span><span class="p">])</span>
        <span class="k">if</span> <span class="n">model_name</span> <span class="o">!=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_selected&#39;</span><span class="p">]:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_model</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session_state</span><span class="o">.</span><span class="n">clear_state</span><span class="p">()</span>
            <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_selected&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">model_name</span>
            <span class="k">if</span> <span class="s1">&#39;chatbot&#39;</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">:</span>
                <span class="k">del</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;chatbot&#39;</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">][</span><span class="n">model_name</span><span class="p">]</span>

        <span class="n">plugin_name</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">sidebar</span><span class="o">.</span><span class="n">multiselect</span><span class="p">(</span>
            <span class="s1">&#39;插件选择&#39;</span><span class="p">,</span>
            <span class="n">options</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;plugin_map&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>
            <span class="n">default</span><span class="o">=</span><span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;plugin_map&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="mi">0</span><span class="p">]],</span>
        <span class="p">)</span>

        <span class="n">plugin_action</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;plugin_map&#39;</span><span class="p">][</span><span class="n">name</span><span class="p">]</span> <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">plugin_name</span>
        <span class="p">]</span>
        <span class="k">if</span> <span class="s1">&#39;chatbot&#39;</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">:</span>
            <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;chatbot&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">_action_executor</span> <span class="o">=</span> <span class="n">ActionExecutor</span><span class="p">(</span>
                <span class="n">actions</span><span class="o">=</span><span class="n">plugin_action</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">st</span><span class="o">.</span><span class="n">sidebar</span><span class="o">.</span><span class="n">button</span><span class="p">(</span><span class="s1">&#39;清空对话&#39;</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="s1">&#39;clear&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">session_state</span><span class="o">.</span><span class="n">clear_state</span><span class="p">()</span>
        <span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">sidebar</span><span class="o">.</span><span class="n">file_uploader</span><span class="p">(</span>
            <span class="s1">&#39;上传文件&#39;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;png&#39;</span><span class="p">,</span> <span class="s1">&#39;jpg&#39;</span><span class="p">,</span> <span class="s1">&#39;jpeg&#39;</span><span class="p">,</span> <span class="s1">&#39;mp4&#39;</span><span class="p">,</span> <span class="s1">&#39;mp3&#39;</span><span class="p">,</span> <span class="s1">&#39;wav&#39;</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">plugin_action</span><span class="p">,</span> <span class="n">uploaded_file</span>

    <span class="k">def</span> <span class="nf">init_model</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">option</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the model based on the selected option.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">option</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">]:</span>
            <span class="k">if</span> <span class="n">option</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s1">&#39;gpt&#39;</span><span class="p">):</span>
                <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">][</span><span class="n">option</span><span class="p">]</span> <span class="o">=</span> <span class="n">GPTAPI</span><span class="p">(</span>
                    <span class="n">model_type</span><span class="o">=</span><span class="n">option</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">][</span><span class="n">option</span><span class="p">]</span> <span class="o">=</span> <span class="n">HFTransformerCasualLM</span><span class="p">(</span>
                    <span class="s1">&#39;/root/model/Shanghai_AI_Laboratory/internlm-chat-7b&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;model_map&#39;</span><span class="p">][</span><span class="n">option</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">initialize_chatbot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">plugin_action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the chatbot with the given model and plugin actions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">ReAct</span><span class="p">(</span>
            <span class="n">llm</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">action_executor</span><span class="o">=</span><span class="n">ActionExecutor</span><span class="p">(</span><span class="n">actions</span><span class="o">=</span><span class="n">plugin_action</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">render_user</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">st</span><span class="o">.</span><span class="n">chat_message</span><span class="p">(</span><span class="s1">&#39;user&#39;</span><span class="p">):</span>
            <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_assistant</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">agent_return</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">st</span><span class="o">.</span><span class="n">chat_message</span><span class="p">(</span><span class="s1">&#39;assistant&#39;</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">action</span> <span class="ow">in</span> <span class="n">agent_return</span><span class="o">.</span><span class="n">actions</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span><span class="n">action</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">render_action</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span><span class="n">agent_return</span><span class="o">.</span><span class="n">response</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_action</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
        <span class="k">with</span> <span class="n">st</span><span class="o">.</span><span class="n">expander</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">type</span><span class="p">,</span> <span class="n">expanded</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
            <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span>
                <span class="s2">&quot;&lt;p style=&#39;text-align: left;display:flex;&#39;&gt; &lt;span style=&#39;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#39;&gt;插    件&lt;/span&gt;&lt;span style=&#39;width:14px;text-align:left;display:block;&#39;&gt;:&lt;/span&gt;&lt;span style=&#39;flex:1;&#39;&gt;&quot;</span>  <span class="c1"># noqa E501</span>
                <span class="o">+</span> <span class="n">action</span><span class="o">.</span><span class="n">type</span> <span class="o">+</span> <span class="s1">&#39;&lt;/span&gt;&lt;/p&gt;&#39;</span><span class="p">,</span>
                <span class="n">unsafe_allow_html</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span>
                <span class="s2">&quot;&lt;p style=&#39;text-align: left;display:flex;&#39;&gt; &lt;span style=&#39;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#39;&gt;思考步骤&lt;/span&gt;&lt;span style=&#39;width:14px;text-align:left;display:block;&#39;&gt;:&lt;/span&gt;&lt;span style=&#39;flex:1;&#39;&gt;&quot;</span>  <span class="c1"># noqa E501</span>
                <span class="o">+</span> <span class="n">action</span><span class="o">.</span><span class="n">thought</span> <span class="o">+</span> <span class="s1">&#39;&lt;/span&gt;&lt;/p&gt;&#39;</span><span class="p">,</span>
                <span class="n">unsafe_allow_html</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">args</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span> <span class="ow">and</span> <span class="s1">&#39;text&#39;</span> <span class="ow">in</span> <span class="n">action</span><span class="o">.</span><span class="n">args</span><span class="p">):</span>
                <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span>
                    <span class="s2">&quot;&lt;p style=&#39;text-align: left;display:flex;&#39;&gt;&lt;span style=&#39;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#39;&gt; 执行内容&lt;/span&gt;&lt;span style=&#39;width:14px;text-align:left;display:block;&#39;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span><span class="p">,</span>  <span class="c1"># noqa E501</span>
                    <span class="n">unsafe_allow_html</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">render_action_results</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">render_action_results</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Render the results of action, including text, images, videos, and</span>
<span class="sd">        audios.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)):</span>
            <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span>
                <span class="s2">&quot;&lt;p style=&#39;text-align: left;display:flex;&#39;&gt;&lt;span style=&#39;font-size:14px;font-weight:600;width:70px;text-align-last: justify;&#39;&gt; 执行结果&lt;/span&gt;&lt;span style=&#39;width:14px;text-align:left;display:block;&#39;&gt;:&lt;/span&gt;&lt;/p&gt;&quot;</span><span class="p">,</span>  <span class="c1"># noqa E501</span>
                <span class="n">unsafe_allow_html</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;text&#39;</span> <span class="ow">in</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">:</span>
                <span class="n">st</span><span class="o">.</span><span class="n">markdown</span><span class="p">(</span>
                    <span class="s2">&quot;&lt;p style=&#39;text-align: left;&#39;&gt;&quot;</span> <span class="o">+</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]</span> <span class="o">+</span>
                    <span class="s1">&#39;&lt;/p&gt;&#39;</span><span class="p">,</span>
                    <span class="n">unsafe_allow_html</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;image&#39;</span> <span class="ow">in</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">:</span>
                <span class="n">image_path</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;image&#39;</span><span class="p">]</span>
                <span class="n">image_data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">image_path</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="n">st</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="n">image_data</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="s1">&#39;Generated Image&#39;</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;video&#39;</span> <span class="ow">in</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">:</span>
                <span class="n">video_data</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;video&#39;</span><span class="p">]</span>
                <span class="n">video_data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">video_data</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="n">st</span><span class="o">.</span><span class="n">video</span><span class="p">(</span><span class="n">video_data</span><span class="p">)</span>
            <span class="k">if</span> <span class="s1">&#39;audio&#39;</span> <span class="ow">in</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">:</span>
                <span class="n">audio_data</span> <span class="o">=</span> <span class="n">action</span><span class="o">.</span><span class="n">result</span><span class="p">[</span><span class="s1">&#39;audio&#39;</span><span class="p">]</span>
                <span class="n">audio_data</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">audio_data</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="n">st</span><span class="o">.</span><span class="n">audio</span><span class="p">(</span><span class="n">audio_data</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="c1"># Initialize Streamlit UI and setup sidebar</span>
    <span class="k">if</span> <span class="s1">&#39;ui&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">:</span>
        <span class="n">session_state</span> <span class="o">=</span> <span class="n">SessionState</span><span class="p">()</span>
        <span class="n">session_state</span><span class="o">.</span><span class="n">init_state</span><span class="p">()</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;ui&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">StreamlitUI</span><span class="p">(</span><span class="n">session_state</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="n">st</span><span class="o">.</span><span class="n">set_page_config</span><span class="p">(</span>
            <span class="n">layout</span><span class="o">=</span><span class="s1">&#39;wide&#39;</span><span class="p">,</span>
            <span class="n">page_title</span><span class="o">=</span><span class="s1">&#39;lagent-web&#39;</span><span class="p">,</span>
            <span class="n">page_icon</span><span class="o">=</span><span class="s1">&#39;./docs/imgs/lagent_icon.png&#39;</span><span class="p">)</span>
        <span class="c1"># st.header(&#39;:robot_face: :blue[Lagent] Web Demo &#39;, divider=&#39;rainbow&#39;)</span>
    <span class="n">model_name</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">plugin_action</span><span class="p">,</span> <span class="n">uploaded_file</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span>
        <span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">setup_sidebar</span><span class="p">()</span>

    <span class="c1"># Initialize chatbot if it is not already initialized</span>
    <span class="c1"># or if the model has changed</span>
    <span class="k">if</span> <span class="s1">&#39;chatbot&#39;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span> <span class="ow">or</span> <span class="n">model</span> <span class="o">!=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span>
            <span class="s1">&#39;chatbot&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">_llm</span><span class="p">:</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;chatbot&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span>
            <span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">initialize_chatbot</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">plugin_action</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">prompt</span><span class="p">,</span> <span class="n">agent_return</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">],</span>
                                    <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;assistant&#39;</span><span class="p">]):</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">render_user</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">render_assistant</span><span class="p">(</span><span class="n">agent_return</span><span class="p">)</span>
    <span class="c1"># User input form at the bottom (this part will be at the bottom)</span>
    <span class="c1"># with st.form(key=&#39;my_form&#39;, clear_on_submit=True):</span>

    <span class="k">if</span> <span class="n">user_input</span> <span class="o">:=</span> <span class="n">st</span><span class="o">.</span><span class="n">chat_input</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">):</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">render_user</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;user&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
        <span class="c1"># Add file uploader to sidebar</span>
        <span class="k">if</span> <span class="n">uploaded_file</span><span class="p">:</span>
            <span class="n">file_bytes</span> <span class="o">=</span> <span class="n">uploaded_file</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
            <span class="n">file_type</span> <span class="o">=</span> <span class="n">uploaded_file</span><span class="o">.</span><span class="n">type</span>
            <span class="k">if</span> <span class="s1">&#39;image&#39;</span> <span class="ow">in</span> <span class="n">file_type</span><span class="p">:</span>
                <span class="n">st</span><span class="o">.</span><span class="n">image</span><span class="p">(</span><span class="n">file_bytes</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="s1">&#39;Uploaded Image&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="s1">&#39;video&#39;</span> <span class="ow">in</span> <span class="n">file_type</span><span class="p">:</span>
                <span class="n">st</span><span class="o">.</span><span class="n">video</span><span class="p">(</span><span class="n">file_bytes</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="s1">&#39;Uploaded Video&#39;</span><span class="p">)</span>
            <span class="k">elif</span> <span class="s1">&#39;audio&#39;</span> <span class="ow">in</span> <span class="n">file_type</span><span class="p">:</span>
                <span class="n">st</span><span class="o">.</span><span class="n">audio</span><span class="p">(</span><span class="n">file_bytes</span><span class="p">,</span> <span class="n">caption</span><span class="o">=</span><span class="s1">&#39;Uploaded Audio&#39;</span><span class="p">)</span>
            <span class="c1"># Save the file to a temporary location and get the path</span>
            <span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="n">uploaded_file</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">file_path</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">tmpfile</span><span class="p">:</span>
                <span class="n">tmpfile</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">file_bytes</span><span class="p">)</span>
            <span class="n">st</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;File saved at: </span><span class="si">{</span><span class="n">file_path</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">user_input</span> <span class="o">=</span> <span class="s1">&#39;我上传了一个图像，路径为: </span><span class="si">{file_path}</span><span class="s1">. </span><span class="si">{user_input}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">file_path</span><span class="o">=</span><span class="n">file_path</span><span class="p">,</span> <span class="n">user_input</span><span class="o">=</span><span class="n">user_input</span><span class="p">)</span>
        <span class="n">agent_return</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;chatbot&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">user_input</span><span class="p">)</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;assistant&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">agent_return</span><span class="p">))</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">agent_return</span><span class="o">.</span><span class="n">inner_steps</span><span class="p">)</span>
        <span class="n">st</span><span class="o">.</span><span class="n">session_state</span><span class="p">[</span><span class="s1">&#39;ui&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">render_assistant</span><span class="p">(</span><span class="n">agent_return</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">root_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="vm">__file__</span><span class="p">)))</span>
    <span class="n">root_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="s1">&#39;tmp_dir&#39;</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">root_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id10">
<h3>3.5 Demo 运行<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h3>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>streamlit<span class="w"> </span>run<span class="w"> </span>/root/code/lagent/examples/react_web_demo.py<span class="w"> </span>--server.address<span class="w"> </span><span class="m">127</span>.0.0.1<span class="w"> </span>--server.port<span class="w"> </span><span class="m">6006</span>
</pre></div>
</div>
<p>用同样的方法我们依然切换到 <code class="docutils literal notranslate"><span class="pre">VScode</span></code> 页面，运行成功后，<span class="xref myst"><strong>查看本教程5.2配置本地端口后</strong></span>，将端口映射到本地。在本地浏览器输入 <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:6006</span></code> 即可。</p>
<p>我们在 <code class="docutils literal notranslate"><span class="pre">Web</span></code> 页面选择 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 模型，等待模型加载完毕后，输入数学问题 已知 <code class="docutils literal notranslate"><span class="pre">2x+3=10</span></code>，求<code class="docutils literal notranslate"><span class="pre">x</span></code> ,此时 <code class="docutils literal notranslate"><span class="pre">InternLM-Chat-7B</span></code> 模型理解题意生成解此题的 <code class="docutils literal notranslate"><span class="pre">Python</span></code> 代码，<code class="docutils literal notranslate"><span class="pre">Lagent</span></code> 调度送入 <code class="docutils literal notranslate"><span class="pre">Python</span></code> 代码解释器求出该问题的解。</p>
<p><img alt="image-7" src="https://github.com/isLinXu/llm-notes/assets/59380685/2b45a9b9-9aa8-4f3a-a75e-4ab3a81cfe71" /></p>
</section>
</section>
<section id="id11">
<h2>4. 浦语·灵笔图文理解创作 Demo<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h2>
<p>本小节我们将使用 <a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 中的 A100(1/4) * 2 机器和 <code class="docutils literal notranslate"><span class="pre">internlm-xcomposer-7b</span></code> 模型部署一个图文理解创作 Demo 。</p>
<section id="id12">
<h3>4.1 环境准备<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h3>
<p>首先在 <a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a> 上选择 A100(1/4)*2 的配置。如下图所示：</p>
<p><img alt="image-8" src="https://github.com/isLinXu/llm-notes/assets/59380685/c8366546-29c9-4efd-81b1-56a02b82dcda" /></p>
<p>接下来打开刚刚租用服务器的 <code class="docutils literal notranslate"><span class="pre">进入开发机</span></code>，并在终端输入 <code class="docutils literal notranslate"><span class="pre">bash</span></code> 命令，进入 <code class="docutils literal notranslate"><span class="pre">conda</span></code> 环境，接下来就是安装依赖。</p>
<p>进入 <code class="docutils literal notranslate"><span class="pre">conda</span></code> 环境之后，使用以下命令从本地克隆一个已有的<code class="docutils literal notranslate"><span class="pre">pytorch</span> <span class="pre">2.0.1</span></code> 的环境</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>/root/share/install_conda_env_internlm_base.sh<span class="w"> </span>xcomposer-demo
</pre></div>
</div>
<p>然后使用以下命令激活环境</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>activate<span class="w"> </span>xcomposer-demo
</pre></div>
</div>
<p>接下来运行以下命令，安装 <code class="docutils literal notranslate"><span class="pre">transformers</span></code>、<code class="docutils literal notranslate"><span class="pre">gradio</span></code> 等依赖包。请严格安装以下版本安装！</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.33.1<span class="w"> </span><span class="nv">timm</span><span class="o">==</span><span class="m">0</span>.4.12<span class="w"> </span><span class="nv">sentencepiece</span><span class="o">==</span><span class="m">0</span>.1.99<span class="w"> </span><span class="nv">gradio</span><span class="o">==</span><span class="m">3</span>.44.4<span class="w"> </span><span class="nv">markdown2</span><span class="o">==</span><span class="m">2</span>.4.10<span class="w"> </span><span class="nv">xlsxwriter</span><span class="o">==</span><span class="m">3</span>.1.2<span class="w"> </span>einops<span class="w"> </span>accelerate
</pre></div>
</div>
</section>
<section id="id13">
<h3>4.2 模型下载<a class="headerlink" href="#id13" title="Permalink to this heading">#</a></h3>
<p><a class="reference external" href="https://studio.intern-ai.org.cn/">InternStudio</a>平台的 <code class="docutils literal notranslate"><span class="pre">share</span></code> 目录下已经为我们准备了全系列的 <code class="docutils literal notranslate"><span class="pre">InternLM</span></code> 模型，所以我们可以直接复制即可。使用如下命令复制：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
cp<span class="w"> </span>-r<span class="w"> </span>/root/share/temp/model_repos/internlm-xcomposer-7b<span class="w"> </span>/root/model/Shanghai_AI_Laboratory
</pre></div>
</div>
<blockquote>
<div><p>-r 选项表示递归地复制目录及其内容</p>
</div></blockquote>
<p>也可以安装 <code class="docutils literal notranslate"><span class="pre">modelscope</span></code>，下载模型的老朋友了</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">modelscope</span><span class="o">==</span><span class="m">1</span>.9.5
</pre></div>
</div>
<p>在 <code class="docutils literal notranslate"><span class="pre">/root/model</span></code> 路径下新建 <code class="docutils literal notranslate"><span class="pre">download.py</span></code> 文件并在其中输入以下内容，并运行 <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">/root/model/download.py</span></code> 执行下载</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">modelscope</span> <span class="kn">import</span> <span class="n">snapshot_download</span><span class="p">,</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="s1">&#39;Shanghai_AI_Laboratory/internlm-xcomposer-7b&#39;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s1">&#39;/root/model&#39;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;master&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id14">
<h3>4.3 代码准备<a class="headerlink" href="#id14" title="Permalink to this heading">#</a></h3>
<p>在 <code class="docutils literal notranslate"><span class="pre">/root/code</span></code> <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">InternLM-XComposer</span></code> 仓库的代码</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/root/code
git<span class="w"> </span>clone<span class="w"> </span>https://gitee.com/internlm/InternLM-XComposer.git
<span class="nb">cd</span><span class="w"> </span>/root/code/InternLM-XComposer
git<span class="w"> </span>checkout<span class="w"> </span>3e8c79051a1356b9c388a6447867355c0634932d<span class="w">  </span><span class="c1"># 最好保证和教程的 commit 版本一致</span>
</pre></div>
</div>
</section>
<section id="id15">
<h3>4.4 Demo 运行<a class="headerlink" href="#id15" title="Permalink to this heading">#</a></h3>
<p>在终端运行以下代码：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span>/root/code/InternLM-XComposer
python<span class="w"> </span>examples/web_demo.py<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>--folder<span class="w"> </span>/root/model/Shanghai_AI_Laboratory/internlm-xcomposer-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_gpus<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--port<span class="w"> </span><span class="m">6006</span>
</pre></div>
</div>
<blockquote>
<div><p>这里 <code class="docutils literal notranslate"><span class="pre">num_gpus</span> <span class="pre">1</span></code> 是因为InternStudio平台对于 <code class="docutils literal notranslate"><span class="pre">A100(1/4)*2</span></code> 识别仍为一张显卡。但如果有小伙伴课后使用两张 3090 来运行此 demo，仍需将 <code class="docutils literal notranslate"><span class="pre">num_gpus</span></code> 设置为 <code class="docutils literal notranslate"><span class="pre">2</span></code> 。</p>
</div></blockquote>
<p><span class="xref myst"><strong>查看本教程5.2配置本地端口后</strong></span>，将端口映射到本地。在本地浏览器输入 <code class="docutils literal notranslate"><span class="pre">http://127.0.0.1:6006</span></code> 即可。我们以<code class="docutils literal notranslate"><span class="pre">又见敦煌</span></code>为提示词，体验图文创作的功能，如下图所示：</p>
<p><img alt="image-9" src="https://github.com/isLinXu/llm-notes/assets/59380685/df4295c2-ccfe-4a3f-83c5-c8667b03b68d" /></p>
<p>接下来，我们可以体验一下图片理解的能力，如下所示~</p>
<p><img alt="image-10" src="https://github.com/isLinXu/llm-notes/assets/59380685/63466f75-f4ad-44df-b902-9f62260f0ae1" /></p>
</section>
</section>
<section id="id16">
<h2>5. 通用环境配置<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h2>
<section id="pipconda">
<h3>5.1 pip、conda 换源<a class="headerlink" href="#pipconda" title="Permalink to this heading">#</a></h3>
<p>更多详细内容可移步至 <a class="reference external" href="https://help.mirrors.cernet.edu.cn/">MirrorZ Help</a> 查看。</p>
<section id="pip">
<h4>5.1.1 pip 换源<a class="headerlink" href="#pip" title="Permalink to this heading">#</a></h4>
<p>临时使用镜像源安装，如下所示：<code class="docutils literal notranslate"><span class="pre">some-package</span></code> 为你需要安装的包名</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-i<span class="w"> </span>https://mirrors.cernet.edu.cn/pypi/web/simple<span class="w"> </span>some-package
</pre></div>
</div>
<p>设置pip默认镜像源，升级 pip 到最新的版本 (&gt;=10.0.0) 后进行配置，如下所示：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>--upgrade<span class="w"> </span>pip
pip<span class="w"> </span>config<span class="w"> </span><span class="nb">set</span><span class="w"> </span>global.index-url<span class="w"> </span>https://mirrors.cernet.edu.cn/pypi/web/simple
</pre></div>
</div>
<p>如果您的 pip 默认源的网络连接较差，临时使用镜像源升级 pip：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>-i<span class="w"> </span>https://mirrors.cernet.edu.cn/pypi/web/simple<span class="w"> </span>--upgrade<span class="w"> </span>pip
</pre></div>
</div>
</section>
<section id="conda">
<h4>5.1.2 conda 换源<a class="headerlink" href="#conda" title="Permalink to this heading">#</a></h4>
<p>镜像站提供了 Anaconda 仓库与第三方源（conda-forge、msys2、pytorch 等），各系统都可以通过修改用户目录下的 <code class="docutils literal notranslate"><span class="pre">.condarc</span></code> 文件来使用镜像站。</p>
<p>不同系统下的 <code class="docutils literal notranslate"><span class="pre">.condarc</span></code> 目录如下：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Linux</span></code>: <code class="docutils literal notranslate"><span class="pre">${HOME}/.condarc</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">macOS</span></code>: <code class="docutils literal notranslate"><span class="pre">${HOME}/.condarc</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Windows</span></code>: <code class="docutils literal notranslate"><span class="pre">C:\Users\&lt;YourUserName&gt;\.condarc</span></code></p></li>
</ul>
<p>注意：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Windows</span></code> 用户无法直接创建名为 <code class="docutils literal notranslate"><span class="pre">.condarc</span></code> 的文件，可先执行 <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">config</span> <span class="pre">--set</span> <span class="pre">show_channel_urls</span> <span class="pre">yes</span></code> 生成该文件之后再修改。</p></li>
</ul>
<p>快速配置</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span><span class="s">&lt;&lt;&#39;EOF&#39; &gt; ~/.condarc</span>
<span class="s">channels:</span>
<span class="s">  - defaults</span>
<span class="s">show_channel_urls: true</span>
<span class="s">default_channels:</span>
<span class="s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main</span>
<span class="s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r</span>
<span class="s">  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2</span>
<span class="s">custom_channels:</span>
<span class="s">  conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>
<span class="s">  pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud</span>
<span class="s">EOF</span>
</pre></div>
</div>
</section>
</section>
<section id="id17">
<h3>5.2 配置本地端口<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h3>
<p>由于服务器通常只暴露了用于安全远程登录的 SSH（Secure Shell）端口，如果需要访问服务器上运行的其他服务（如 web 应用）的特定端口，需要一种特殊的设置。我们可以通过使用SSH隧道的方法，将服务器上的这些特定端口映射到本地计算机的端口。这样做的步骤如下：</p>
<p>首先我们需要配置一下本地的 <code class="docutils literal notranslate"><span class="pre">SSH</span> <span class="pre">Key</span></code> ，我们这里以 <code class="docutils literal notranslate"><span class="pre">Windows</span></code> 为例。</p>
<p>步骤①：在本地机器上打开 <code class="docutils literal notranslate"><span class="pre">Power</span> <span class="pre">Shell</span></code> 终端。在终端中，运行以下命令来生成 SSH 密钥对：（如下图所示）</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh-keygen<span class="w"> </span>-t<span class="w"> </span>rsa
</pre></div>
</div>
<p><img alt="image-13" src="https://github.com/isLinXu/llm-notes/assets/59380685/9f31c6ca-0586-49dc-aee2-9139cd60b190" /></p>
<p>步骤②： 您将被提示选择密钥文件的保存位置，默认情况下是在 <code class="docutils literal notranslate"><span class="pre">~/.ssh/</span></code> 目录中。按 <code class="docutils literal notranslate"><span class="pre">Enter</span></code> 键接受默认值或输入自定义路径。</p>
<p>步骤③：公钥默认存储在 <code class="docutils literal notranslate"><span class="pre">~/.ssh/id_rsa.pub</span></code>，可以通过系统自带的 <code class="docutils literal notranslate"><span class="pre">cat</span></code> 工具查看文件内容：（如下图所示）</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>~<span class="se">\.</span>ssh<span class="se">\i</span>d_rsa.pub
</pre></div>
</div>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">~</span></code> 是用户主目录的简写，<code class="docutils literal notranslate"><span class="pre">.ssh</span></code> 是SSH配置文件的默认存储目录，<code class="docutils literal notranslate"><span class="pre">id_rsa.pub</span></code> 是 SSH 公钥文件的默认名称。所以，<code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">~\.ssh\id_rsa.pub</span></code> 的意思是查看用户主目录下的 <code class="docutils literal notranslate"><span class="pre">.ssh</span></code> 目录中的 <code class="docutils literal notranslate"><span class="pre">id_rsa.pub</span></code> 文件的内容。</p>
</div></blockquote>
<p><img alt="image-14" src="https://github.com/isLinXu/llm-notes/assets/59380685/1c2319c5-6600-4503-80bb-159aa901ef78" /></p>
<p>步骤④：将公钥复制到剪贴板中，然后回到 <code class="docutils literal notranslate"><span class="pre">InternStudio</span></code> 控制台，点击配置 SSH Key。如下图所示：</p>
<p><img alt="image-14" src="https://github.com/isLinXu/llm-notes/assets/59380685/ba876941-9e69-46f3-a8c2-9d265664c3c7" /></p>
<p>步骤⑤：将刚刚复制的公钥添加进入即可。</p>
<p><img alt="image-16" src="https://github.com/isLinXu/llm-notes/assets/59380685/6d9f9778-39ed-47b6-959b-7ec694b0e488" /></p>
<p>步骤⑥：在本地终端输入以下指令 <code class="docutils literal notranslate"><span class="pre">.6006</span></code> 是在服务器中打开的端口，而 <code class="docutils literal notranslate"><span class="pre">33090</span></code> 是根据开发机的端口进行更改。如下图所示：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>-CNg<span class="w"> </span>-L<span class="w"> </span><span class="m">6006</span>:127.0.0.1:6006<span class="w"> </span>root@ssh.intern-ai.org.cn<span class="w"> </span>-p<span class="w"> </span><span class="m">33090</span>
</pre></div>
</div>
<p><img alt="image-17" src="https://github.com/isLinXu/llm-notes/assets/59380685/88ff3f7a-845d-434b-9224-23544f17e698" /></p>
</section>
<section id="id18">
<h3>5.3 模型下载<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h3>
<section id="hugging-face">
<h4>5.3.1 Hugging Face<a class="headerlink" href="#hugging-face" title="Permalink to this heading">#</a></h4>
<p>使用 Hugging Face 官方提供的 <code class="docutils literal notranslate"><span class="pre">huggingface-cli</span></code> 命令行工具。安装依赖:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>huggingface_hub
</pre></div>
</div>
<p>然后新建 python 文件，填入以下代码，运行即可。</p>
<ul class="simple">
<li><p>resume-download：断点续下</p></li>
<li><p>local-dir：本地存储路径。（linux 环境下需要填写绝对路径）</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># 下载模型</span>
<span class="n">os</span><span class="o">.</span><span class="n">system</span><span class="p">(</span><span class="s1">&#39;huggingface-cli download --resume-download internlm/internlm-chat-7b --local-dir your_path&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>以下内容将展示使用 <code class="docutils literal notranslate"><span class="pre">huggingface_hub</span></code> 下载模型中的部分文件</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span> 
<span class="kn">from</span> <span class="nn">huggingface_hub</span> <span class="kn">import</span> <span class="n">hf_hub_download</span>  <span class="c1"># Load model directly </span>

<span class="n">hf_hub_download</span><span class="p">(</span><span class="n">repo_id</span><span class="o">=</span><span class="s2">&quot;internlm/internlm-7b&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;config.json&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="modelscope">
<h4>5.3.2 ModelScope<a class="headerlink" href="#modelscope" title="Permalink to this heading">#</a></h4>
<p>使用 <code class="docutils literal notranslate"><span class="pre">modelscope</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">snapshot_download</span></code> 函数下载模型，第一个参数为模型名称，参数 <code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> 为模型的下载路径。</p>
<p>注意：<code class="docutils literal notranslate"><span class="pre">cache_dir</span></code> 最好为绝对路径。</p>
<p>安装依赖：</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">modelscope</span><span class="o">==</span><span class="m">1</span>.9.5
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.35.2
</pre></div>
</div>
<p>在当前目录下新建 python 文件，填入以下代码，运行即可。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">modelscope</span> <span class="kn">import</span> <span class="n">snapshot_download</span><span class="p">,</span> <span class="n">AutoModel</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="n">model_dir</span> <span class="o">=</span> <span class="n">snapshot_download</span><span class="p">(</span><span class="s1">&#39;Shanghai_AI_Laboratory/internlm-chat-7b&#39;</span><span class="p">,</span> <span class="n">cache_dir</span><span class="o">=</span><span class="s1">&#39;your path&#39;</span><span class="p">,</span> <span class="n">revision</span><span class="o">=</span><span class="s1">&#39;master&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="openxlab">
<h4>5.3.3 OpenXLab<a class="headerlink" href="#openxlab" title="Permalink to this heading">#</a></h4>
<p>OpenXLab 可以通过指定模型仓库的地址，以及需要下载的文件的名称，文件所需下载的位置等，直接下载模型权重文件。</p>
<p>使用python脚本下载模型首先要安装依赖，安装代码如下：<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">-U</span> <span class="pre">openxlab</span></code> 安装完成后使用 download 函数导入模型中心的模型。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openxlab.model</span> <span class="kn">import</span> <span class="n">download</span>
<span class="n">download</span><span class="p">(</span><span class="n">model_repo</span><span class="o">=</span><span class="s1">&#39;OpenLMLab/InternLM-7b&#39;</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;InternLM-7b&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">=</span><span class="s1">&#39;your local path&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="id19">
<h2>6. 课后作业<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h2>
<p>提交方式：在各个班级对应的 GitHub Discussion 帖子中进行提交。</p>
<p><strong>基础作业：</strong></p>
<ul class="simple">
<li><p>使用 InternLM-Chat-7B 模型生成 300 字的小故事（需截图）。</p></li>
<li><p>熟悉 hugging face 下载功能，使用 <code class="docutils literal notranslate"><span class="pre">huggingface_hub</span></code> python 包，下载 <code class="docutils literal notranslate"><span class="pre">InternLM-20B</span></code> 的 config.json 文件到本地（需截图下载过程）。</p></li>
</ul>
<p><strong>进阶作业（可选做）</strong></p>
<ul class="simple">
<li><p>完成浦语·灵笔的图文理解及创作部署（需截图）</p></li>
<li><p>完成 Lagent 工具调用 Demo 创作部署（需截图）</p></li>
</ul>
<p><strong>整体实训营项目：</strong></p>
<p>时间周期：即日起致课程结束</p>
<p>即日开始可以在班级群中随机组队完成一个大作业项目，一些可提供的选题如下：</p>
<ul class="simple">
<li><p>人情世故大模型：一个帮助用户撰写新年祝福文案的人情事故大模型</p></li>
<li><p>中小学数学大模型：一个拥有一定数学解题能力的大模型</p></li>
<li><p>心理大模型：一个治愈的心理大模型</p></li>
<li><p>工具调用类项目：结合 Lagent 构建数据集训练 InternLM 模型，支持对 MMYOLO 等工具的调用</p></li>
</ul>
<p>其他基于书生·浦语工具链的小项目都在范围内，欢迎大家充分发挥想象力。</p>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="1-%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">1-书生浦语大模型全链路开源开放体系</p>
      </div>
    </a>
    <a class="right-next"
       href="3-%E5%9F%BA%E4%BA%8EInternLM%E5%92%8CLangchain%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">3-基于InternLM和Langchain搭建你的知识库</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#internlm">1 大模型及 InternLM 模型简介</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">1.1 什么是大模型？</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1.2 InternLM 模型全链条开源</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#internlm-chat-7b-demo">2 InternLM-Chat-7B 智能对话 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.3 代码准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.4 终端运行</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#web-demo">2.5 web demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lagent-demo">3 Lagent 智能体工具调用 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">3.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">3.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lagent">3.3 Lagent 安装</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">3.4 修改代码</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">3.5 Demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">4. 浦语·灵笔图文理解创作 Demo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">4.1 环境准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">4.2 模型下载</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">4.3 代码准备</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">4.4 Demo 运行</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">5. 通用环境配置</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipconda">5.1 pip、conda 换源</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pip">5.1.1 pip 换源</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#conda">5.1.2 conda 换源</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">5.2 配置本地端口</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">5.3 模型下载</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hugging-face">5.3.1 Hugging Face</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#modelscope">5.3.2 ModelScope</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#openxlab">5.3.3 OpenXLab</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">6. 课后作业</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>