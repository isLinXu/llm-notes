

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>5-LMDeploy大模型量化部署实践 &#8212; 大语言模型学习笔记</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=12da95d707ffb74b382d" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=12da95d707ffb74b382d" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'InternLM/5-LMDeploy大模型量化部署实践';</script>
    <link rel="shortcut icon" href="../_static/panda.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="6-OpenCompass大模型评测解读及实战指南" href="6-OpenCompass%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97.html" />
    <link rel="prev" title="4-XTuner大模型单卡低成本微调实战" href="4-XTuner%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%95%E5%8D%A1%E4%BD%8E%E6%88%90%E6%9C%AC%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">目录</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">书生·浦语大模型实战营</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1-%E4%B9%A6%E7%94%9F%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A8%E9%93%BE%E8%B7%AF%E5%BC%80%E6%BA%90%E5%BC%80%E6%94%BE%E4%BD%93%E7%B3%BB.html">1-书生浦语大模型全链路开源开放体系</a></li>
<li class="toctree-l2"><a class="reference internal" href="2-%E8%BD%BB%E6%9D%BE%E5%88%86%E9%92%9F%E7%8E%A9%E8%BD%AC%E4%B9%A6%E7%94%9F%C2%B7%E6%B5%A6%E8%AF%AD%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%B6%A3%E5%91%B3Demo.html">2-轻松分钟玩转书生·浦语大模型趣味Demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="3-%E5%9F%BA%E4%BA%8EInternLM%E5%92%8CLangchain%E6%90%AD%E5%BB%BA%E4%BD%A0%E7%9A%84%E7%9F%A5%E8%AF%86%E5%BA%93.html">3-基于InternLM和Langchain搭建你的知识库</a></li>
<li class="toctree-l2"><a class="reference internal" href="4-XTuner%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%95%E5%8D%A1%E4%BD%8E%E6%88%90%E6%9C%AC%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98.html">4-XTuner大模型单卡低成本微调实战</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">5-LMDeploy大模型量化部署实践</a></li>

<li class="toctree-l2"><a class="reference internal" href="6-OpenCompass%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97.html">6-OpenCompass大模型评测解读及实战指南</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes/edit/main/InternLM/5-LMDeploy大模型量化部署实践.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/isLinXu/llm-notes/issues/new?title=Issue%20on%20page%20%2FInternLM/5-LMDeploy大模型量化部署实践.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/InternLM/5-LMDeploy大模型量化部署实践.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>5-LMDeploy大模型量化部署实践</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">5-LMDeploy大模型量化部署实践</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">LMDeploy 的量化和部署</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1 环境配置</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2 服务部署</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.1 模型转换</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.1.1 在线转换</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.1.2 离线转换</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind">2.2  TurboMind 推理+命令行本地对话</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind-api">2.3 TurboMind推理+API服务</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demo">2.4 网页 Demo 演示</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2.4.1 TurboMind 服务作为后端</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.4.2 TurboMind 推理作为后端</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind-python">2.5 TurboMind 推理 + Python 代码集成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">2.6 这么多，头秃，有没有最佳实践</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">2.6.1 方案实践</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">2.6.2 模型配置实践</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3 模型量化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kv-cache">3.1 KV Cache 量化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.1.1 量化步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.1.2 量化效果</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#w4a16">3.2 W4A16 量化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">3.2.1 量化步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.2.2 量化效果</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">3.3 最佳实践</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">参考资料</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tritonserver">附录1：TritonServer 作为推理引擎</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">TritonServer环境配置</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tritonserver-api">TritonServer推理+API服务</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">TritonServer 服务作为后端</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="lmdeploy">
<h1>5-LMDeploy大模型量化部署实践<a class="headerlink" href="#lmdeploy" title="Permalink to this heading">#</a></h1>
<hr class="docutils" />
<p><img alt="cover" src="https://github.com/isLinXu/llm-notes/assets/59380685/ced0ec54-f481-49e0-a249-2f6691aa4a8f" /></p>
</section>
<section id="id1">
<h1>LMDeploy 的量化和部署<a class="headerlink" href="#id1" title="Permalink to this heading">#</a></h1>
<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
<ul class="simple">
<li><p><span class="xref myst">1 环境配置</span></p></li>
<li><p><span class="xref myst">2 服务部署</span></p>
<ul>
<li><p><span class="xref myst">2.1 模型转换</span></p>
<ul>
<li><p><span class="xref myst">2.1.1 在线转换</span></p></li>
<li><p><span class="xref myst">2.1.2 离线转换</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">2.2  TurboMind推理+命令行本地对话</span></p></li>
<li><p><span class="xref myst">2.3 TurboMind推理+API服务</span></p></li>
<li><p><span class="xref myst">2.4 网页Demo演示</span></p>
<ul>
<li><p><span class="xref myst">2.4.1 TurboMind服务作为后端</span></p></li>
<li><p><span class="xref myst">2.4.2 TurboMind推理作为后端</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">2.5 TurboMind推理+Python代码集成</span></p></li>
<li><p><span class="xref myst">2.6 这么多，头秃，有没有最佳实践</span></p>
<ul>
<li><p><span class="xref myst">2.6.1 方案实践</span></p></li>
<li><p><span class="xref myst">2.6.2 模型配置实践</span></p></li>
</ul>
</li>
</ul>
</li>
<li><p><span class="xref myst">3 模型量化</span></p>
<ul>
<li><p><span class="xref myst">3.1 KV Cache 量化</span></p>
<ul>
<li><p><span class="xref myst">3.1.1 量化步骤</span></p></li>
<li><p><span class="xref myst">3.1.2 量化效果</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">3.2 W4A16 量化</span></p>
<ul>
<li><p><span class="xref myst">3.2.1 量化步骤</span></p></li>
<li><p><span class="xref myst">3.2.2 量化效果</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">3.3 最佳实践</span></p></li>
</ul>
</li>
<li><p><span class="xref myst">参考资料</span></p></li>
<li><p><span class="xref myst">附录1：TritonServer作为推理引擎</span></p>
<ul>
<li><p><span class="xref myst">TritonServer环境配置</span></p></li>
<li><p><span class="xref myst">TritonServer推理+API服务</span></p></li>
<li><p><span class="xref myst">TritonServer服务作为后端</span></p></li>
</ul>
</li>
</ul>
<!-- END doctoc generated TOC please keep comment here to allow auto update -->
<section id="id2">
<h2>1 环境配置<a class="headerlink" href="#id2" title="Permalink to this heading">#</a></h2>
<p>首先我们可以使用 <code class="docutils literal notranslate"><span class="pre">vgpu-smi</span> </code> 查看显卡资源使用情况。</p>
<img width="770" alt="0" src="https://github.com/isLinXu/llm-notes/assets/59380685/3e4b813c-5637-4c3c-93a2-3698dc725111">
<p>大家可以使用系统给的 vscode 进行后面的开发。分别点击下图「1」和「2」的位置，就会在下方显示终端。</p>
<img width="1434" alt="add0" src="https://github.com/isLinXu/llm-notes/assets/59380685/adc75d4c-09ad-4ce5-a744-f331ab6ab7ab">
<p>可以点击终端（TERMINAL）窗口右侧的「+」号创建新的终端窗口。大家可以新开一个窗口，执行下面的命令实时观察 GPU 资源的使用情况。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>watch<span class="w"> </span>vgpu-smi
</pre></div>
</div>
<p>结果如下图所示，该窗口会实时检测 GPU 卡的使用情况。</p>
<img width="1431" alt="add1" src="https://github.com/isLinXu/llm-notes/assets/59380685/979517b5-3271-44cb-9f0f-5256d61ef5b6">
<p>接下来我们切换到刚刚的终端（就是上图右边的那个「bash」，下面的「watch」就是监控的终端），创建部署和量化需要的环境。建议大家使用官方提供的环境，使用 conda 直接复制。</p>
<p>这里 <code class="docutils literal notranslate"><span class="pre">/share/conda_envs</span></code> 目录下的环境是官方未大家准备好的基础环境，因为该目录是共享只读的，而我们后面需要在此基础上安装新的软件包，所以需要复制到我们自己的 conda 环境（该环境下我们是可写的）。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>CONDA_ENV_NAME<span class="w"> </span>--clone<span class="w"> </span>/share/conda_envs/internlm-base
</pre></div>
</div>
<p>我们取 <code class="docutils literal notranslate"><span class="pre">CONDA_ENV_NAME</span></code> 为 <code class="docutils literal notranslate"><span class="pre">lmdeploy</span></code>，复制完成后，可以在本地查看环境。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda<span class="w"> </span>env<span class="w"> </span>list
</pre></div>
</div>
<p>结果如下所示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># conda environments:</span>
<span class="c1">#</span>
base<span class="w">                  </span>*<span class="w">  </span>/root/.conda
lmdeploy<span class="w">                 </span>/root/.conda/envs/lmdeploy
</pre></div>
</div>
<p>然后激活环境。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>conda<span class="w"> </span>activate<span class="w"> </span>lmdeploy
</pre></div>
</div>
<p>注意，环境激活后，左边会显示当前（也就是 <code class="docutils literal notranslate"><span class="pre">lmdeploy</span></code>）的环境名称，如下图所示。</p>
<img width="522" alt="1" src="https://github.com/isLinXu/llm-notes/assets/59380685/6082f68e-790d-433a-a228-4e391e5e3fcc">
<p>可以进入Python检查一下 PyTorch 和 lmdeploy 的版本。由于 PyTorch 在官方提供的环境里，我们应该可以看到版本显示，而 lmdeploy 需要我们自己安装，此处应该会提示“没有这个包”，如下图所示。</p>
<img width="683" alt="2" src="https://github.com/isLinXu/llm-notes/assets/59380685/31c273ea-5b01-47e7-9ed4-d3fbbf17ac73">
<p>lmdeploy 没有安装，我们接下来手动安装一下，建议安装最新的稳定版。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="s1">&#39;lmdeploy[all]==v0.1.0&#39;</span>
</pre></div>
</div>
<p>由于默认安装的是 runtime 依赖包，但是我们这里还需要部署和量化，所以，这里选择 <code class="docutils literal notranslate"><span class="pre">[all]</span></code>。然后可以再检查一下 lmdeploy 包，如下图所示。</p>
<img width="822" alt="add3" src="https://github.com/isLinXu/llm-notes/assets/59380685/412dc33a-eab6-4166-b25d-fcba7d12d5d4">
<p>如果遇到类似 <code class="docutils literal notranslate"> <span class="pre">ModuleNotFoundError:</span> <span class="pre">No</span> <span class="pre">module</span> <span class="pre">named</span> <span class="pre">'packaging'</span></code> 这样的错误，大家可以手动先安装一下。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>packaging
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">packaging</span></code> 表示缺少的包名。</p>
<p>基础环境到这里就配置好了。</p>
</section>
<section id="id3">
<h2>2 服务部署<a class="headerlink" href="#id3" title="Permalink to this heading">#</a></h2>
<p>这一部分主要涉及本地推理和部署。我们先看一张图。</p>
<p><img alt="lmdeploy drawio" src="https://github.com/isLinXu/llm-notes/assets/59380685/b0214fd1-6900-46fe-82f2-f9a50be542b9" /></p>
<p>我们把从架构上把整个服务流程分成下面几个模块。</p>
<ul class="simple">
<li><p>模型推理/服务。主要提供模型本身的推理，一般来说可以和具体业务解耦，专注模型推理本身性能的优化。可以以模块、API等多种方式提供。</p></li>
<li><p>Client。可以理解为前端，与用户交互的地方。</p></li>
<li><p>API Server。一般作为前端的后端，提供与产品和服务相关的数据和功能支持。</p></li>
</ul>
<p>值得说明的是，以上的划分是一个相对完整的模型，但在实际中这并不是绝对的。比如可以把“模型推理”和“API Server”合并，有的甚至是三个流程打包在一起提供服务。</p>
<p>接下来，我们看一下lmdeploy提供的部署功能。</p>
<section id="id4">
<h3>2.1 模型转换<a class="headerlink" href="#id4" title="Permalink to this heading">#</a></h3>
<p>使用 TurboMind 推理模型需要先将模型转化为 TurboMind 的格式，目前支持在线转换和离线转换两种形式。在线转换可以直接加载 Huggingface 模型，离线转换需需要先保存模型再加载。</p>
<p>TurboMind 是一款关于 LLM 推理的高效推理引擎，基于英伟达的 <a class="reference external" href="https://github.com/NVIDIA/FasterTransformer">FasterTransformer</a> 研发而成。它的主要功能包括：LLaMa 结构模型的支持，persistent batch 推理模式和可扩展的 KV 缓存管理器。</p>
<section id="id5">
<h4>2.1.1 在线转换<a class="headerlink" href="#id5" title="Permalink to this heading">#</a></h4>
<p>lmdeploy 支持直接读取 Huggingface 模型权重，目前共支持三种类型：</p>
<ul class="simple">
<li><p>在 huggingface.co 上面通过 lmdeploy 量化的模型，如 <a class="reference external" href="https://huggingface.co/lmdeploy/llama2-chat-70b-4bit">llama2-70b-4bit</a>, <a class="reference external" href="https://huggingface.co/internlm/internlm-chat-20b-4bit">internlm-chat-20b-4bit</a></p></li>
<li><p>huggingface.co 上面其他 LM 模型，如 Qwen/Qwen-7B-Chat</p></li>
</ul>
<p>示例如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 需要能访问 Huggingface 的网络环境</span>
lmdeploy<span class="w"> </span>chat<span class="w"> </span>turbomind<span class="w"> </span>internlm/internlm-chat-20b-4bit<span class="w"> </span>--model-name<span class="w"> </span>internlm-chat-20b
lmdeploy<span class="w"> </span>chat<span class="w"> </span>turbomind<span class="w"> </span>Qwen/Qwen-7B-Chat<span class="w"> </span>--model-name<span class="w"> </span>qwen-7b
</pre></div>
</div>
<p>上面两行命令分别展示了如何直接加载 Huggingface 的模型，第一条命令是加载使用 lmdeploy 量化的版本，第二条命令是加载其他 LLM 模型。</p>
<p>我们也可以直接启动本地的 Huggingface 模型，如下所示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lmdeploy<span class="w"> </span>chat<span class="w"> </span>turbomind<span class="w"> </span>/share/temp/model_repos/internlm-chat-7b/<span class="w">  </span>--model-name<span class="w"> </span>internlm-chat-7b
</pre></div>
</div>
<p>以上命令都会启动一个本地对话界面，通过 Bash 可以与 LLM 进行对话。</p>
</section>
<section id="id6">
<h4>2.1.2 离线转换<a class="headerlink" href="#id6" title="Permalink to this heading">#</a></h4>
<p>离线转换需要在启动服务之前，将模型转为 lmdeploy TurboMind  的格式，如下所示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 转换模型（FastTransformer格式） TurboMind</span>
lmdeploy<span class="w"> </span>convert<span class="w"> </span>internlm-chat-7b<span class="w"> </span>/path/to/internlm-chat-7b
</pre></div>
</div>
<p>这里我们使用官方提供的模型文件，就在用户根目录执行，如下所示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lmdeploy<span class="w"> </span>convert<span class="w"> </span>internlm-chat-7b<span class="w">  </span>/root/share/temp/model_repos/internlm-chat-7b/
</pre></div>
</div>
<p>执行完成后将会在当前目录生成一个 <code class="docutils literal notranslate"><span class="pre">workspace</span></code> 的文件夹。这里面包含的就是 TurboMind 和 Triton “模型推理”需要到的文件。</p>
<p>目录如下图所示。</p>
<p><img alt="lmdeploy drawio" src="https://github.com/isLinXu/llm-notes/assets/59380685/21408012-ad36-4c0d-934c-a7db0363196c" /></p>
<p><code class="docutils literal notranslate"><span class="pre">weights</span></code> 和 <code class="docutils literal notranslate"><span class="pre">tokenizer</span></code> 目录分别放的是拆分后的参数和 Tokenizer。如果我们进一步查看 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 的目录，就会发现参数是按层和模块拆开的，如下图所示。</p>
<img width="798" alt="5" src="https://github.com/isLinXu/llm-notes/assets/59380685/33a296f8-8c8c-48e5-bd24-c6649cdc7039">
<p>每一份参数第一个 0 表示“层”的索引，后面的那个0表示 Tensor 并行的索引，因为我们只有一张卡，所以被拆分成 1 份。如果有两张卡可以用来推理，则会生成0和1两份，也就是说，会把同一个参数拆成两份。比如 <code class="docutils literal notranslate"><span class="pre">layers.0.attention.w_qkv.0.weight</span></code> 会变成 <code class="docutils literal notranslate"><span class="pre">layers.0.attention.w_qkv.0.weight</span></code> 和 <code class="docutils literal notranslate"><span class="pre">layers.0.attention.w_qkv.1.weight</span></code>。执行 <code class="docutils literal notranslate"><span class="pre">lmdeploy</span> <span class="pre">convert</span></code> 命令时，可以通过 <code class="docutils literal notranslate"><span class="pre">--tp</span></code> 指定（tp 表示 tensor parallel），该参数默认值为1（也就是一张卡）。</p>
<p><strong>关于Tensor并行</strong></p>
<p>Tensor并行一般分为行并行或列并行，原理如下图所示。</p>
<p><img alt="6" src="https://github.com/isLinXu/llm-notes/assets/59380685/698ae759-a5ce-49b5-82d2-9ad74611da3f" /></p>
<p align="center">列并行<p>
<p><img alt="6" src="https://github.com/isLinXu/llm-notes/assets/59380685/afbfd28f-6a16-42b8-a400-097f2a8da6c2" /></p>
<p align="center">行并行<p>
<p>简单来说，就是把一个大的张量（参数）分到多张卡上，分别计算各部分的结果，然后再同步汇总。</p>
</section>
</section>
<section id="turbomind">
<h3>2.2  TurboMind 推理+命令行本地对话<a class="headerlink" href="#turbomind" title="Permalink to this heading">#</a></h3>
<p>模型转换完成后，我们就具备了使用模型推理的条件，接下来就可以进行真正的模型推理环节。</p>
<p>我们先尝试本地对话（<code class="docutils literal notranslate"><span class="pre">Bash</span> <span class="pre">Local</span> <span class="pre">Chat</span></code>），下面用（Local Chat 表示）在这里其实是跳过 API Server 直接调用 TurboMind。简单来说，就是命令行代码直接执行 TurboMind。所以说，实际和前面的架构图是有区别的。</p>
<p>这里支持多种方式运行，比如Turbomind、PyTorch、DeepSpeed。但 PyTorch 和 DeepSpeed 调用的其实都是 Huggingface 的 Transformers 包，PyTorch表示原生的 Transformer 包，DeepSpeed 表示使用了 DeepSpeed 作为推理框架。Pytorch/DeepSpeed 目前功能都比较弱，不具备生产能力，不推荐使用。</p>
<p>执行命令如下。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Turbomind + Bash Local Chat</span>
lmdeploy<span class="w"> </span>chat<span class="w"> </span>turbomind<span class="w"> </span>./workspace
</pre></div>
</div>
<p>启动后就可以和它进行对话了，如下图所示。</p>
<img width="862" alt="8" src="https://github.com/isLinXu/llm-notes/assets/59380685/952e7a73-85e6-4158-a8ae-31fd1095ef57">
<p>输入后两次回车，退出时输入<code class="docutils literal notranslate"><span class="pre">exit</span></code> 回车两次即可。此时，Server 就是本地跑起来的模型（TurboMind），命令行可以看作是前端。</p>
</section>
<section id="turbomind-api">
<h3>2.3 TurboMind推理+API服务<a class="headerlink" href="#turbomind-api" title="Permalink to this heading">#</a></h3>
<p>在上面的部分我们尝试了直接用命令行启动 Client，接下来我们尝试如何运用 lmdepoy 进行服务化。</p>
<p>”模型推理/服务“目前提供了 Turbomind 和 TritonServer 两种服务化方式。此时，Server 是 TurboMind 或 TritonServer，API Server 可以提供对外的 API 服务。我们推荐使用 TurboMind，TritonServer 使用方式详见《附录1》。</p>
<p>首先，通过下面命令启动服务。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ApiServer+Turbomind   api_server =&gt; AsyncEngine =&gt; TurboMind</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>api_server<span class="w"> </span>./workspace<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_name<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_port<span class="w"> </span><span class="m">23333</span><span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--instance_num<span class="w"> </span><span class="m">64</span><span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--tp<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>上面的参数中 <code class="docutils literal notranslate"><span class="pre">server_name</span></code> 和 <code class="docutils literal notranslate"><span class="pre">server_port</span></code> 分别表示服务地址和端口，<code class="docutils literal notranslate"><span class="pre">tp</span></code> 参数我们之前已经提到过了，表示 Tensor 并行。还剩下一个 <code class="docutils literal notranslate"><span class="pre">instance_num</span></code> 参数，表示实例数，可以理解成 Batch 的大小。执行后如下图所示。</p>
<img width="841" alt="11" src="https://github.com/isLinXu/llm-notes/assets/59380685/7cf8dbb5-5300-4c64-9f47-5b77c5048442">
<p>然后，我们可以新开一个窗口，执行下面的 Client 命令。如果使用官方机器，可以打开 vscode 的 Terminal，执行下面的命令。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ChatApiClient+ApiServer（注意是http协议，需要加http）</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>api_client<span class="w"> </span>http://localhost:23333
</pre></div>
</div>
<p>如下图所示。</p>
<img width="1238" alt="12" src="https://github.com/isLinXu/llm-notes/assets/59380685/86091b66-7c23-4fe2-8da3-b18277c8fc18">
<p>当然，刚刚我们启动的是 API Server，自然也有相应的接口。可以直接打开 <code class="docutils literal notranslate"><span class="pre">http://{host}:23333</span></code> 查看，如下图所示。</p>
<img width="1386" alt="13" src="https://github.com/isLinXu/llm-notes/assets/59380685/e7d8f715-a148-4742-a5f6-df3a13943e7c">
<blockquote>
<div><p>注意，这一步由于 Server 在远程服务器上，所以本地需要做一下 ssh 转发才能直接访问（与第一部分操作一样），命令如下：</p>
<p>ssh -CNg -L 23333:127.0.0.1:23333 root&#64;ssh.intern-ai.org.cn -p &lt;你的ssh端口号&gt;</p>
<p>而执行本命令需要添加本机公钥，公钥添加后等待几分钟即可生效。ssh 端口号就是下面图片里的 33087。</p>
<img width="1068" alt="20" src="https://github.com/isLinXu/llm-notes/assets/59380685/a4400040-1c0b-460e-be3b-199a4c6ef4ce">
</div></blockquote>
<p>这里一共提供了 4 个 HTTP 的接口，任何语言都可以对其进行调用，我们以 <code class="docutils literal notranslate"><span class="pre">v1/chat/completions</span></code> 接口为例，简单试一下。</p>
<p>接口请求参数如下：</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">  </span><span class="nt">&quot;model&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;internlm-chat-7b&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;messages&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;写一首春天的诗&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;temperature&quot;</span><span class="p">:</span><span class="w"> </span><span class="mf">0.7</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;top_p&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;n&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;max_tokens&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;stop&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;stream&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;presence_penalty&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;frequency_penalty&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;user&quot;</span><span class="p">:</span><span class="w"> </span><span class="s2">&quot;string&quot;</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;repetition_penalty&quot;</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;renew_session&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="p">,</span>
<span class="w">  </span><span class="nt">&quot;ignore_eos&quot;</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span>
<span class="p">}</span>
</pre></div>
</div>
<p>请求结果如下。</p>
<img width="1240" alt="16" src="https://github.com/isLinXu/llm-notes/assets/59380685/d9d3a966-13c3-486a-93a5-b1bfca39e70b">
</section>
<section id="demo">
<h3>2.4 网页 Demo 演示<a class="headerlink" href="#demo" title="Permalink to this heading">#</a></h3>
<p>这一部分主要是将 Gradio 作为前端 Demo 演示。在上一节的基础上，我们不执行后面的 <code class="docutils literal notranslate"><span class="pre">api_client</span></code> 或 <code class="docutils literal notranslate"><span class="pre">triton_client</span></code>，而是执行 <code class="docutils literal notranslate"><span class="pre">gradio</span></code>。</p>
<blockquote>
<div><p>由于 Gradio 需要本地访问展示界面，因此也需要通过 ssh 将数据转发到本地。命令如下：</p>
<p>ssh -CNg -L 6006:127.0.0.1:6006 root&#64;ssh.intern-ai.org.cn -p &lt;你的 ssh 端口号&gt;</p>
</div></blockquote>
<section id="id7">
<h4>2.4.1 TurboMind 服务作为后端<a class="headerlink" href="#id7" title="Permalink to this heading">#</a></h4>
<p>API Server 的启动和上一节一样，这里直接启动作为前端的 Gradio。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradio+ApiServer。必须先开启 Server，此时 Gradio 为 Client</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>gradio<span class="w"> </span>http://0.0.0.0:23333<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_name<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_port<span class="w"> </span><span class="m">6006</span><span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--restful_api<span class="w"> </span>True
</pre></div>
</div>
<p>结果如下图所示。</p>
<img width="1240" alt="16" src="https://github.com/isLinXu/llm-notes/assets/59380685/2d907b20-5982-4413-a2ba-4cbcd2dfa7fb">
</section>
<section id="id8">
<h4>2.4.2 TurboMind 推理作为后端<a class="headerlink" href="#id8" title="Permalink to this heading">#</a></h4>
<p>当然，Gradio 也可以直接和 TurboMind 连接，如下所示。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradio+Turbomind(local)</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>gradio<span class="w"> </span>./workspace
</pre></div>
</div>
<p>可以直接启动 Gradio，此时没有 API Server，TurboMind 直接与 Gradio 通信。如下图所示。</p>
<img width="1296" alt="19" src="https://github.com/isLinXu/llm-notes/assets/59380685/35ad1e0e-a43c-4c1f-b16b-df498b0950e1">
</section>
</section>
<section id="turbomind-python">
<h3>2.5 TurboMind 推理 + Python 代码集成<a class="headerlink" href="#turbomind-python" title="Permalink to this heading">#</a></h3>
<p>前面介绍的都是通过 API 或某种前端与”模型推理/服务“进行交互，lmdeploy 还支持 Python 直接与 TurboMind 进行交互，如下所示。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lmdeploy</span> <span class="kn">import</span> <span class="n">turbomind</span> <span class="k">as</span> <span class="n">tm</span>

<span class="c1"># load model</span>
<span class="n">model_path</span> <span class="o">=</span> <span class="s2">&quot;/root/share/temp/model_repos/internlm-chat-7b/&quot;</span>
<span class="n">tm_model</span> <span class="o">=</span> <span class="n">tm</span><span class="o">.</span><span class="n">TurboMind</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">model_name</span><span class="o">=</span><span class="s1">&#39;internlm-chat-20b&#39;</span><span class="p">)</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">tm_model</span><span class="o">.</span><span class="n">create_instance</span><span class="p">()</span>

<span class="c1"># process query</span>
<span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;你好啊兄嘚&quot;</span>
<span class="n">prompt</span> <span class="o">=</span> <span class="n">tm_model</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">get_prompt</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tm_model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

<span class="c1"># inference</span>
<span class="k">for</span> <span class="n">outputs</span> <span class="ow">in</span> <span class="n">generator</span><span class="o">.</span><span class="n">stream_infer</span><span class="p">(</span>
        <span class="n">session_id</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="p">[</span><span class="n">input_ids</span><span class="p">]):</span>
    <span class="n">res</span><span class="p">,</span> <span class="n">tokens</span> <span class="o">=</span> <span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">tm_model</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">res</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
<p>在上面的代码中，我们首先加载模型，然后构造输入，最后执行推理。</p>
<p>加载模型可以显式指定模型路径，也可以直接指定 Huggingface 的 repo_id，还可以使用上面生成过的 <code class="docutils literal notranslate"><span class="pre">workspace</span></code>。这里的 <code class="docutils literal notranslate"><span class="pre">tm.TurboMind</span></code> 其实是对 C++ TurboMind 的封装。</p>
<p>构造输入这里主要是把用户的 query 构造成 InternLLM 支持的输入格式，比如上面的例子中， <code class="docutils literal notranslate"><span class="pre">query</span></code> 是“你好啊兄嘚”，构造好的 Prompt 如下所示。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">&lt;|System|&gt;:You are an AI assistant whose name is InternLM (书生·浦语).</span>
<span class="sd">- InternLM (书生·浦语) is a conversational language model that is developed by Shanghai AI Laboratory (上海人工智能实验室). It is designed to be helpful, honest, and harmless.</span>
<span class="sd">- InternLM (书生·浦语) can understand and communicate fluently in the language chosen by the user such as English and 中文.</span>

<span class="sd">&lt;|User|&gt;:你好啊兄嘚</span>
<span class="sd">&lt;|Bot|&gt;:</span>
<span class="sd">&quot;&quot;&quot;</span>
</pre></div>
</div>
<p>Prompt 其实就是增加了 <code class="docutils literal notranslate"><span class="pre">&lt;|System|&gt;</span></code> 消息和 <code class="docutils literal notranslate"><span class="pre">&lt;|User|&gt;</span></code> 消息（即用户的 <code class="docutils literal notranslate"><span class="pre">query</span></code>），以及一个 <code class="docutils literal notranslate"><span class="pre">&lt;|Bot|&gt;</span></code> 的标记，表示接下来该模型输出响应了。最终输出的响应内容如下所示。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;你好啊，有什么我可以帮助你的吗？&quot;</span>
</pre></div>
</div>
</section>
<section id="id9">
<h3>2.6 这么多，头秃，有没有最佳实践<a class="headerlink" href="#id9" title="Permalink to this heading">#</a></h3>
<section id="id10">
<h4>2.6.1 方案实践<a class="headerlink" href="#id10" title="Permalink to this heading">#</a></h4>
<p>必——须——有！</p>
<p>首先说 “模型推理/服务”，推荐使用 TurboMind，使用简单，性能良好，相关的 Benchmark 对比如下。</p>
<p><img alt="add4" src="https://github.com/isLinXu/llm-notes/assets/59380685/ec763760-98d2-44b5-9cd6-29437b84a94c" /></p>
<p>上面的性能对比包括两个场景：</p>
<ul class="simple">
<li><p>场景一（前4张图）：固定的输入、输出 token 数（分别1和2048），测试Token输出吞吐量（output token throughput）。</p></li>
<li><p>场景二（第5张图）：使用真实数据，测试吞吐量（request throughput）。</p></li>
</ul>
<p>场景一中，BatchSize=64时，TurboMind 的吞吐量超过 2000 token/s，整体比 DeepSpeed 提升约 5% - 15%；BatchSize=32时，比 Huggingface 的Transformers 提升约 3 倍；其他BatchSize时 TurboMind 也表现出优异的性能。</p>
<p>场景二中，对比了 TurboMind 和 vLLM 在真实数据上的吞吐量（request throughput）指标，TurboMind 的效率比 vLLM 高 30%。</p>
<p>大家不妨亲自使用本地对话（Local Chat）感受一下性能差别（2.2 节），也可以执行我们提供的 <code class="docutils literal notranslate"><span class="pre">infer_compare.py</span></code> 脚本，示例如下。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 执行 Huggingface 的 Transformer</span>
python<span class="w"> </span>infer_compare.py<span class="w"> </span>hf
<span class="c1"># 执行LMDeploy</span>
python<span class="w"> </span>infer_compare.py<span class="w"> </span>lmdeploy
</pre></div>
</div>
<p>LMDeploy应该是Transformers的3-5倍左右。</p>
<p>后面的 API 服务和 Client 就得分场景了。</p>
<ul class="simple">
<li><p>我想对外提供类似 OpenAI 那样的 HTTP 接口服务。推荐使用 TurboMind推理 + API 服务（2.3）。</p></li>
<li><p>我想做一个演示 Demo，Gradio 无疑是比 Local Chat 更友好的。推荐使用 TurboMind 推理作为后端的Gradio进行演示（2.4.2）。</p></li>
<li><p>我想直接在自己的 Python 项目中使用大模型功能。推荐使用 TurboMind推理 + Python（2.5）。</p></li>
<li><p>我想在自己的其他非 Python 项目中使用大模型功能。推荐直接通过 HTTP 接口调用服务。也就是用本列表第一条先启动一个 HTTP API 服务，然后在项目中直接调用接口。</p></li>
<li><p>我的项目是 C++ 写的，为什么不能直接用 TurboMind 的 C++ 接口？！必须可以！大佬可以右上角叉掉这个窗口啦。</p></li>
</ul>
</section>
<section id="id11">
<h4>2.6.2 模型配置实践<a class="headerlink" href="#id11" title="Permalink to this heading">#</a></h4>
<p>不知道大家还有没有印象，在离线转换（2.1.2）一节，我们查看了 <code class="docutils literal notranslate"><span class="pre">weights</span></code> 的目录，里面存放的是模型按层、按并行卡拆分的参数，不过还有一个文件 <code class="docutils literal notranslate"><span class="pre">config.ini</span></code> 并不是模型参数，它里面存的主要是模型相关的配置信息。下面是一个示例。</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[llama]</span>
<span class="na">model_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">internlm-chat-7b</span>
<span class="na">tensor_para_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">head_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">kv_head_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">vocab_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">103168</span>
<span class="na">num_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">inter_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">11008</span>
<span class="na">norm_eps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1e-06</span>
<span class="na">attn_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
<span class="na">start_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">end_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2</span>
<span class="na">session_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2056</span>
<span class="na">weight_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">fp16</span>
<span class="na">rotary_embedding</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
<span class="na">rope_theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">10000.0</span>
<span class="na">size_per_head</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
<span class="na">group_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
<span class="na">max_batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">64</span>
<span class="na">max_context_token_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">step_length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">cache_max_entry_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.5</span>
<span class="na">cache_block_seq_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
<span class="na">cache_chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">use_context_fmha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">quant_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
<span class="na">max_position_embeddings</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2048</span>
<span class="na">rope_scaling_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.0</span>
<span class="na">use_logn_attn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
</pre></div>
</div>
<p>其中，模型属性相关的参数不可更改，主要包括下面这些。</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">model_name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">llama2</span>
<span class="na">head_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">kv_head_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">vocab_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">103168</span>
<span class="na">num_layer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">32</span>
<span class="na">inter_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">11008</span>
<span class="na">norm_eps</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1e-06</span>
<span class="na">attn_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
<span class="na">start_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">end_id</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2</span>
<span class="na">rotary_embedding</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
<span class="na">rope_theta</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">10000.0</span>
<span class="na">size_per_head</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
</pre></div>
</div>
<p>和数据类型相关的参数也不可更改，主要包括两个。</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">weight_type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">fp16</span>
<span class="na">group_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">weight_type</span></code> 表示权重的数据类型。目前支持 fp16 和 int4。int4 表示 4bit 权重。当 <code class="docutils literal notranslate"><span class="pre">weight_type</span></code> 为 4bit 权重时，<code class="docutils literal notranslate"><span class="pre">group_size</span></code> 表示 <code class="docutils literal notranslate"><span class="pre">awq</span></code> 量化权重时使用的 group 大小。</p>
<p>剩余参数包括下面几个。</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">tensor_para_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">session_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2056</span>
<span class="na">max_batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">64</span>
<span class="na">max_context_token_num</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">step_length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">cache_max_entry_count</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.5</span>
<span class="na">cache_block_seq_len</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">128</span>
<span class="na">cache_chunk_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">use_context_fmha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">1</span>
<span class="na">quant_policy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
<span class="na">max_position_embeddings</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">2048</span>
<span class="na">rope_scaling_factor</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0.0</span>
<span class="na">use_logn_attn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">0</span>
</pre></div>
</div>
<p>一般情况下，我们并不需要对这些参数进行修改，但有时候为了满足特定需要，可能需要调整其中一部分配置值。这里主要介绍三个可能需要调整的参数。</p>
<ul class="simple">
<li><p>KV int8 开关：</p>
<ul>
<li><p>对应参数为 <code class="docutils literal notranslate"><span class="pre">quant_policy</span></code>，默认值为 0，表示不适用 KV Cache，如果需要开启，则将该参数设置为 4。</p></li>
<li><p>KV Cache 是对序列生成过程中的 K 和 V 进行量化，用以节省显存。我们下一部分会介绍具体的量化过程。</p></li>
<li><p>当显存不足，或序列比较长时，建议打开此开关。</p></li>
</ul>
</li>
<li><p>外推能力开关：</p>
<ul>
<li><p>对应参数为 <code class="docutils literal notranslate"><span class="pre">rope_scaling_factor</span></code>，默认值为 0.0，表示不具备外推能力，设置为 1.0，可以开启 RoPE 的 Dynamic NTK 功能，支持长文本推理。另外，<code class="docutils literal notranslate"><span class="pre">use_logn_attn</span></code> 参数表示 Attention 缩放，默认值为 0，如果要开启，可以将其改为 1。</p></li>
<li><p>外推能力是指推理时上下文的长度超过训练时的最大长度时模型生成的能力。如果没有外推能力，当推理时上下文长度超过训练时的最大长度，效果会急剧下降。相反，则下降不那么明显，当然如果超出太多，效果也会下降的厉害。</p></li>
<li><p>当推理文本非常长（明显超过了训练时的最大长度）时，建议开启外推能力。</p></li>
</ul>
</li>
<li><p>批处理大小：</p>
<ul>
<li><p>对应参数为 <code class="docutils literal notranslate"><span class="pre">max_batch_size</span></code>，默认为 64，也就是我们在 API Server 启动时的 <code class="docutils literal notranslate"><span class="pre">instance_num</span></code> 参数。</p></li>
<li><p>该参数值越大，吞度量越大（同时接受的请求数），但也会占用更多显存。</p></li>
<li><p>建议根据请求量和最大的上下文长度，按实际情况调整。</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="id12">
<h2>3 模型量化<a class="headerlink" href="#id12" title="Permalink to this heading">#</a></h2>
<p>本部分内容主要介绍如何对模型进行量化。主要包括 KV Cache 量化和模型参数量化。总的来说，量化是一种以参数或计算中间结果精度下降换空间节省（以及同时带来的性能提升）的策略。</p>
<p>正式介绍 LMDeploy 量化方案前，需要先介绍两个概念：</p>
<ul class="simple">
<li><p>计算密集（compute-bound）: 指推理过程中，绝大部分时间消耗在数值计算上；针对计算密集型场景，可以通过使用更快的硬件计算单元来提升计算速。</p></li>
<li><p>访存密集（memory-bound）: 指推理过程中，绝大部分时间消耗在数据读取上；针对访存密集型场景，一般通过减少访存次数、提高计算访存比或降低访存量来优化。</p></li>
</ul>
<p>常见的 LLM 模型由于 Decoder Only 架构的特性，实际推理时大多数的时间都消耗在了逐 Token 生成阶段（Decoding 阶段），是典型的访存密集型场景。</p>
<p>那么，如何优化 LLM 模型推理中的访存密集问题呢？ 我们可以使用 <strong>KV Cache 量化</strong>和 <strong>4bit Weight Only 量化（W4A16）</strong>。KV Cache 量化是指将逐 Token（Decoding）生成过程中的上下文 K 和 V 中间结果进行 INT8 量化（计算时再反量化），以降低生成过程中的显存占用。4bit Weight 量化，将 FP16 的模型权重量化为 INT4，Kernel 计算时，访存量直接降为 FP16 模型的 1/4，大幅降低了访存成本。Weight Only 是指仅量化权重，数值计算依然采用 FP16（需要将 INT4 权重反量化）。</p>
<section id="kv-cache">
<h3>3.1 KV Cache 量化<a class="headerlink" href="#kv-cache" title="Permalink to this heading">#</a></h3>
<section id="id13">
<h4>3.1.1 量化步骤<a class="headerlink" href="#id13" title="Permalink to this heading">#</a></h4>
<p>KV Cache 量化是将已经生成序列的 KV 变成 Int8，使用过程一共包括三步：</p>
<p>第一步：计算 minmax。主要思路是通过计算给定输入样本在每一层不同位置处计算结果的统计情况。</p>
<ul class="simple">
<li><p>对于 Attention 的 K 和 V：取每个 Head 各自维度在所有Token的最大、最小和绝对值最大值。对每一层来说，上面三组值都是 <code class="docutils literal notranslate"><span class="pre">(num_heads,</span> <span class="pre">head_dim)</span></code> 的矩阵。这里的统计结果将用于本小节的 KV Cache。</p></li>
<li><p>对于模型每层的输入：取对应维度的最大、最小、均值、绝对值最大和绝对值均值。每一层每个位置的输入都有对应的统计值，它们大多是 <code class="docutils literal notranslate"><span class="pre">(hidden_dim,</span> <span class="pre">)</span></code> 的一维向量，当然在 FFN 层由于结构是先变宽后恢复，因此恢复的位置维度并不相同。这里的统计结果用于下个小节的模型参数量化，主要用在缩放环节（回顾PPT内容）。</p></li>
</ul>
<p>第一步执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 计算 minmax</span>
lmdeploy<span class="w"> </span>lite<span class="w"> </span>calibrate<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w">  </span>/root/share/temp/model_repos/internlm-chat-7b/<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--calib_dataset<span class="w"> </span><span class="s2">&quot;c4&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--calib_samples<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--calib_seqlen<span class="w"> </span><span class="m">2048</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--work_dir<span class="w"> </span>./quant_output
</pre></div>
</div>
<p>在这个命令行中，会选择 128 条输入样本，每条样本长度为 2048，数据集选择 C4，输入模型后就会得到上面的各种统计值。值得说明的是，如果显存不足，可以适当调小 samples 的数量或 sample 的长度。</p>
<blockquote>
<div><p>这一步由于默认需要从 Huggingface 下载数据集，国内经常不成功。所以我们导出了需要的数据，大家需要对读取数据集的代码文件做一下替换。共包括两步：</p>
<ul class="simple">
<li><p>第一步：复制 <code class="docutils literal notranslate"><span class="pre">calib_dataloader.py</span></code> 到安装目录替换该文件：<code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">/root/share/temp/datasets/c4/calib_dataloader.py</span>&#160; <span class="pre">/root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/</span></code></p></li>
<li><p>第二步：将用到的数据集（c4）复制到下面的目录：<code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">-r</span> <span class="pre">/root/share/temp/datasets/c4/</span> <span class="pre">/root/.cache/huggingface/datasets/</span></code></p></li>
</ul>
</div></blockquote>
<p>第二步：通过 minmax 获取量化参数。主要就是利用下面这个公式，获取每一层的 K V 中心值（zp）和缩放值（scale）。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">zp</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span>min+max<span class="o">)</span><span class="w"> </span>/<span class="w"> </span><span class="m">2</span>
<span class="nv">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">(</span>max-min<span class="o">)</span><span class="w"> </span>/<span class="w"> </span><span class="m">255</span>
quant:<span class="w"> </span><span class="nv">q</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>round<span class="o">(</span><span class="w"> </span><span class="o">(</span>f-zp<span class="o">)</span><span class="w"> </span>/<span class="w"> </span>scale<span class="o">)</span>
dequant:<span class="w"> </span><span class="nv">f</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>q<span class="w"> </span>*<span class="w"> </span>scale<span class="w"> </span>+<span class="w"> </span>zp
</pre></div>
</div>
<p>有这两个值就可以进行量化和解量化操作了。具体来说，就是对历史的 K 和 V 存储 quant 后的值，使用时在 dequant。</p>
<p>第二步的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 通过 minmax 获取量化参数</span>
lmdeploy<span class="w"> </span>lite<span class="w"> </span>kv_qparams<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--work_dir<span class="w"> </span>./quant_output<span class="w">  </span><span class="se">\</span>
<span class="w">  </span>--turbomind_dir<span class="w"> </span>workspace/triton_models/weights/<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--kv_sym<span class="w"> </span>False<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--num_tp<span class="w"> </span><span class="m">1</span>
</pre></div>
</div>
<p>在这个命令中，<code class="docutils literal notranslate"><span class="pre">num_tp</span></code> 的含义前面介绍过，表示 Tensor 的并行数。每一层的中心值和缩放值会存储到 <code class="docutils literal notranslate"><span class="pre">workspace</span></code> 的参数目录中以便后续使用。<code class="docutils literal notranslate"><span class="pre">kv_sym</span></code> 为 <code class="docutils literal notranslate"><span class="pre">True</span></code> 时会使用另一种（对称）量化方法，它用到了第一步存储的绝对值最大值，而不是最大值和最小值。</p>
<p>第三步：修改配置。也就是修改 <code class="docutils literal notranslate"><span class="pre">weights/config.ini</span></code> 文件，这个我们在《2.6.2 模型配置实践》中已经提到过了（KV int8 开关），只需要把 <code class="docutils literal notranslate"><span class="pre">quant_policy</span></code> 改为 4 即可。</p>
<p>这一步需要额外说明的是，如果用的是 TurboMind1.0，还需要修改参数 <code class="docutils literal notranslate"><span class="pre">use_context_fmha</span></code>，将其改为 0。</p>
<p>接下来就可以正常运行前面的各种服务了，只不过咱们现在可是用上了 KV Cache 量化，能更省（运行时）显存了。</p>
</section>
<section id="id14">
<h4>3.1.2 量化效果<a class="headerlink" href="#id14" title="Permalink to this heading">#</a></h4>
<p>官方给出了 <a class="reference external" href="https://huggingface.co/internlm/internlm-chat-7b">internlm-chat-7b</a> 模型在 KV Cache 量化前后的显存对比情况，如下表所示。</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>batch_size</p></th>
<th class="head"><p>fp16 memory(MiB)</p></th>
<th class="head"><p>int8 memory(MiB)</p></th>
<th class="head"><p>diff(MiB)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>8</p></td>
<td><p>22337</p></td>
<td><p>18241</p></td>
<td><p>-4096</p></td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><p>30593</p></td>
<td><p>22369</p></td>
<td><p>-8224</p></td>
</tr>
<tr class="row-even"><td><p>32</p></td>
<td><p>47073</p></td>
<td><p>30625</p></td>
<td><p>-16448</p></td>
</tr>
<tr class="row-odd"><td><p>48</p></td>
<td><p>63553</p></td>
<td><p>38881</p></td>
<td><p>-24672</p></td>
</tr>
</tbody>
</table>
<p>可以看出，KV Cache 可以节约大约 20% 的显存。</p>
<p>同时，还在 <a class="reference external" href="https://github.com/open-compass/opencompass">opencompass</a> 平台上测试了量化前后的精准度（Accuracy）对比情况，如下表所示。</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>task</p></th>
<th class="head"><p>dataset</p></th>
<th class="head"><p>metric</p></th>
<th class="head"><p>int8</p></th>
<th class="head"><p>fp16</p></th>
<th class="head"><p>diff</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Language</p></td>
<td><p>winogrande</p></td>
<td><p>accuracy</p></td>
<td><p>60.77</p></td>
<td><p>61.48</p></td>
<td><p>-0.71</p></td>
</tr>
<tr class="row-odd"><td><p>Knowledge</p></td>
<td><p>nq</p></td>
<td><p>score</p></td>
<td><p>2.69</p></td>
<td><p>2.60</p></td>
<td><p>+0.09</p></td>
</tr>
<tr class="row-even"><td><p>Reasoning</p></td>
<td><p>gsm8k</p></td>
<td><p>accuracy</p></td>
<td><p>33.28</p></td>
<td><p>34.72</p></td>
<td><p>-1.44</p></td>
</tr>
<tr class="row-odd"><td><p>Reasoning</p></td>
<td><p>bbh</p></td>
<td><p>naive_average</p></td>
<td><p>20.12</p></td>
<td><p>20.51</p></td>
<td><p>-0.39</p></td>
</tr>
<tr class="row-even"><td><p>Understanding</p></td>
<td><p>openbookqa_fact</p></td>
<td><p>accuracy</p></td>
<td><p>82.40</p></td>
<td><p>82.20</p></td>
<td><p>+0.20</p></td>
</tr>
<tr class="row-odd"><td><p>Understanding</p></td>
<td><p>eprstmt-dev</p></td>
<td><p>accuracy</p></td>
<td><p>90.62</p></td>
<td><p>88.75</p></td>
<td><p>+1.87</p></td>
</tr>
<tr class="row-even"><td><p>Safety</p></td>
<td><p>crows_pairs</p></td>
<td><p>accuracy</p></td>
<td><p>32.56</p></td>
<td><p>31.43</p></td>
<td><p>+1.13</p></td>
</tr>
</tbody>
</table>
<p>可以看出，精度不仅没有明显下降，相反在不少任务上还有一定的提升。可能得原因是，量化会导致一定的误差，有时候这种误差可能会减少模型对训练数据的拟合，从而提高泛化性能。量化可以被视为引入轻微噪声的正则化方法。或者，也有可能量化后的模型正好对某些数据集具有更好的性能。</p>
<p>总结一下，KV Cache 量化既能明显降低显存占用，还有可能同时带来精准度（Accuracy）的提升。</p>
</section>
</section>
<section id="w4a16">
<h3>3.2 W4A16 量化<a class="headerlink" href="#w4a16" title="Permalink to this heading">#</a></h3>
<section id="id15">
<h4>3.2.1 量化步骤<a class="headerlink" href="#id15" title="Permalink to this heading">#</a></h4>
<p>W4A16 中的 A 是指 Activation，保持 FP16，其余参数进行 4bit 量化。使用过程也可以看作是三步。</p>
<blockquote>
<div><p>在深度学习中，W（Weight）一般用于定义不同层神经元之间的连接强度。Bias 是在通过激活函数之前添加到输入的加权总和中的附加数值。它们有助于控制神经元的输出，并为模型的学习过程提供灵活性。Bias 可以被认为是一种将激活函数向左或向右移动的方法，允许模型在输入数据中学习更复杂的模式和关系。所以此处的 A 是指 Bias 参数。</p>
</div></blockquote>
<p>第一步：同 1.3.1，不再赘述。</p>
<p>第二步：量化权重模型。利用第一步得到的统计值对参数进行量化，具体又包括两小步：</p>
<ul class="simple">
<li><p>缩放参数。主要是性能上的考虑（回顾 PPT）。</p></li>
<li><p>整体量化。</p></li>
</ul>
<p>第二步的执行命令如下：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 量化权重模型</span>
lmdeploy<span class="w"> </span>lite<span class="w"> </span>auto_awq<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model<span class="w">  </span>/root/share/temp/model_repos/internlm-chat-7b/<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--w_bits<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--w_group_size<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--work_dir<span class="w"> </span>./quant_output<span class="w"> </span>
</pre></div>
</div>
<p>命令中 <code class="docutils literal notranslate"><span class="pre">w_bits</span></code> 表示量化的位数，<code class="docutils literal notranslate"><span class="pre">w_group_size</span></code> 表示量化分组统计的尺寸，<code class="docutils literal notranslate"><span class="pre">work_dir</span></code> 是量化后模型输出的位置。这里需要特别说明的是，因为没有 <code class="docutils literal notranslate"><span class="pre">torch.int4</span></code>，所以实际存储时，8个 4bit 权重会被打包到一个 int32 值中。所以，如果你把这部分量化后的参数加载进来就会发现它们是 int32 类型的。</p>
<p>最后一步：转换成 TurboMind 格式。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># 转换模型的layout，存放在默认路径 ./workspace 下</span>
lmdeploy<span class="w"> </span>convert<span class="w">  </span>internlm-chat-7b<span class="w"> </span>./quant_output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-format<span class="w"> </span>awq<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-size<span class="w"> </span><span class="m">128</span>
</pre></div>
</div>
<p>这个 <code class="docutils literal notranslate"><span class="pre">group-size</span></code> 就是上一步的那个 <code class="docutils literal notranslate"><span class="pre">w_group_size</span></code>。如果不想和之前的 <code class="docutils literal notranslate"><span class="pre">workspace</span></code> 重复，可以指定输出目录：<code class="docutils literal notranslate"><span class="pre">--dst_path</span></code>，比如：</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>lmdeploy<span class="w"> </span>convert<span class="w">  </span>internlm-chat-7b<span class="w"> </span>./quant_output<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model-format<span class="w"> </span>awq<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group-size<span class="w"> </span><span class="m">128</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dst_path<span class="w"> </span>./workspace_quant
</pre></div>
</div>
<p>接下来和上一节一样，可以正常运行前面的各种服务了，不过咱们现在用的是量化后的模型。</p>
<p>最后再补充一点，量化模型和 KV Cache 量化也可以一起使用，以达到最大限度节省显存。</p>
</section>
<section id="id16">
<h4>3.2.2 量化效果<a class="headerlink" href="#id16" title="Permalink to this heading">#</a></h4>
<p>官方在 NVIDIA GeForce RTX 4090 上测试了 4-bit 的 Llama-2-7B-chat 和 Llama-2-13B-chat 模型的 token 生成速度。测试配置为 BatchSize = 1，prompt_tokens=1，completion_tokens=512，结果如下表所示。</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>model</p></th>
<th class="head"><p>llm-awq</p></th>
<th class="head"><p>mlc-llm</p></th>
<th class="head"><p>turbomind</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama-2-7B-chat</p></td>
<td><p>112.9</p></td>
<td><p>159.4</p></td>
<td><p>206.4</p></td>
</tr>
<tr class="row-odd"><td><p>Llama-2-13B-chat</p></td>
<td><p>N/A</p></td>
<td><p>90.7</p></td>
<td><p>115.8</p></td>
</tr>
</tbody>
</table>
<p>可以看出，TurboMind 相比其他框架速度优势非常显著，比 mlc-llm 快了将近 30%。</p>
<p>另外，也测试了 TurboMind 在不同精度和上下文长度下的显存占用情况，如下表所示。</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>model(context length)</p></th>
<th class="head"><p>16bit(2048)</p></th>
<th class="head"><p>4bit(2048)</p></th>
<th class="head"><p>16bit(4096)</p></th>
<th class="head"><p>4bit(4096)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Llama-2-7B-chat</p></td>
<td><p>15.1</p></td>
<td><p>6.3</p></td>
<td><p>16.2</p></td>
<td><p>7.5</p></td>
</tr>
<tr class="row-odd"><td><p>Llama-2-13B-chat</p></td>
<td><p>OOM</p></td>
<td><p>10.3</p></td>
<td><p>OOM</p></td>
<td><p>12.0</p></td>
</tr>
</tbody>
</table>
<p>可以看出，4bit 模型可以降低 50-60% 的显存占用，效果非常明显。</p>
<p>总而言之，W4A16 参数量化后能极大地降低显存，同时相比其他框架推理速度具有明显优势。</p>
</section>
</section>
<section id="id17">
<h3>3.3 最佳实践<a class="headerlink" href="#id17" title="Permalink to this heading">#</a></h3>
<p>本节是针对《模型量化》部分的最佳实践。</p>
<p>首先我们需要明白一点，服务部署和量化是没有直接关联的，量化的最主要目的是降低显存占用，主要包括两方面的显存：模型参数和中间过程计算结果。前者对应《3.2 W4A16 量化》，后者对应《3.1 KV Cache 量化》。</p>
<p>量化在降低显存的同时，一般还能带来性能的提升，因为更小精度的浮点数要比高精度的浮点数计算效率高，而整型要比浮点数高很多。</p>
<p>所以我们的建议是：在各种配置下尝试，看效果能否满足需要。这一般需要在自己的数据集上进行测试。具体步骤如下。</p>
<ul class="simple">
<li><p>Step1：优先尝试正常（非量化）版本，评估效果。</p>
<ul>
<li><p>如果效果不行，需要尝试更大参数模型或者微调。</p></li>
<li><p>如果效果可以，跳到下一步。</p></li>
</ul>
</li>
<li><p>Step2：尝试正常版本+KV Cache 量化，评估效果。</p>
<ul>
<li><p>如果效果不行，回到上一步。</p></li>
<li><p>如果效果可以，跳到下一步。</p></li>
</ul>
</li>
<li><p>Step3：尝试量化版本，评估效果。</p>
<ul>
<li><p>如果效果不行，回到上一步。</p></li>
<li><p>如果效果可以，跳到下一步。</p></li>
</ul>
</li>
<li><p>Step4：尝试量化版本+ KV Cache 量化，评估效果。</p>
<ul>
<li><p>如果效果不行，回到上一步。</p></li>
<li><p>如果效果可以，使用方案。</p></li>
</ul>
</li>
</ul>
<p>简单流程如下图所示。</p>
<p><img alt="quant drawio" src="https://github.com/isLinXu/llm-notes/assets/59380685/c423a36d-faff-424c-af70-b335435865f8" /></p>
<p>另外需要补充说明的是，使用哪种量化版本、开启哪些功能，除了上述流程外，<strong>还需要考虑框架、显卡的支持情况</strong>，比如有些框架可能不支持 W4A16 的推理，那即便转换好了也用不了。</p>
<p>根据实践经验，一般情况下：</p>
<ul class="simple">
<li><p>精度越高，显存占用越多，推理效率越低，但一般效果较好。</p></li>
<li><p>Server 端推理一般用非量化版本或半精度、BF16、Int8 等精度的量化版本，比较少使用更低精度的量化版本。</p></li>
<li><p>端侧推理一般都使用量化版本，且大多是低精度的量化版本。这主要是因为计算资源所限。</p></li>
</ul>
<p>以上是针对项目开发情况，如果是自己尝试（玩儿）的话：</p>
<ul class="simple">
<li><p>如果资源足够（有GPU卡很重要），那就用非量化的正常版本。</p></li>
<li><p>如果没有 GPU 卡，只有 CPU（不管什么芯片），那还是尝试量化版本。</p></li>
<li><p>如果生成文本长度很长，显存不够，就开启 KV Cache。</p></li>
</ul>
<p>建议大家根据实际情况灵活选择方案。</p>
</section>
</section>
<section id="id18">
<h2>参考资料<a class="headerlink" href="#id18" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/InternLM/lmdeploy/">InternLM/lmdeploy: LMDeploy is a toolkit for compressing, deploying, and serving LLMs.</a></p></li>
<li><p><a class="reference external" href="https://zhuanlan.zhihu.com/p/665725861">仅需一块 3090 显卡，高效部署 InternLM-20B 模型 - 知乎</a></p></li>
</ul>
</section>
<section id="tritonserver">
<h2>附录1：TritonServer 作为推理引擎<a class="headerlink" href="#tritonserver" title="Permalink to this heading">#</a></h2>
<section id="id19">
<h3>TritonServer环境配置<a class="headerlink" href="#id19" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>注意：本部分内容仅支持物理机上执行，不支持虚拟主机。</p>
</div></blockquote>
<p>使用 Triton Server 需要安装一下 Docker 及其他依赖。</p>
<p>先装一些基本的依赖。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>apt-get<span class="w"> </span>update
apt-get<span class="w"> </span>install<span class="w"> </span>cmake<span class="w"> </span>sudo<span class="w"> </span>-y
</pre></div>
</div>
<p>然后是 Docker 安装。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add Docker&#39;s official GPG key:</span>
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>ca-certificates<span class="w"> </span>curl<span class="w"> </span>gnupg
sudo<span class="w"> </span>install<span class="w"> </span>-m<span class="w"> </span><span class="m">0755</span><span class="w"> </span>-d<span class="w"> </span>/etc/apt/keyrings
curl<span class="w"> </span>-fsSL<span class="w"> </span>https://download.docker.com/linux/ubuntu/gpg<span class="w"> </span><span class="p">|</span><span class="w"> </span>sudo<span class="w"> </span>gpg<span class="w"> </span>--dearmor<span class="w"> </span>-o<span class="w"> </span>/etc/apt/keyrings/docker.gpg
sudo<span class="w"> </span>chmod<span class="w"> </span>a+r<span class="w"> </span>/etc/apt/keyrings/docker.gpg

<span class="c1"># Add the repository to Apt sources:</span>
<span class="nb">echo</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span><span class="s2">&quot;deb [arch=</span><span class="k">$(</span>dpkg<span class="w"> </span>--print-architecture<span class="k">)</span><span class="s2"> signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \</span>
<span class="s2">  </span><span class="k">$(</span>.<span class="w"> </span>/etc/os-release<span class="w"> </span><span class="o">&amp;&amp;</span><span class="w"> </span><span class="nb">echo</span><span class="w"> </span><span class="s2">&quot;</span><span class="nv">$VERSION_CODENAME</span><span class="s2">&quot;</span><span class="k">)</span><span class="s2"> stable&quot;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>sudo<span class="w"> </span>tee<span class="w"> </span>/etc/apt/sources.list.d/docker.list<span class="w"> </span>&gt;<span class="w"> </span>/dev/null
sudo<span class="w"> </span>apt-get<span class="w"> </span>update

<span class="c1"># install</span>
sudo<span class="w"> </span>apt-get<span class="w"> </span>install<span class="w"> </span>docker-ce<span class="w"> </span>docker-ce-cli<span class="w"> </span>containerd.io<span class="w"> </span>docker-buildx-plugin<span class="w"> </span>docker-compose-plugin
</pre></div>
</div>
<p>安装后我们跑一个 HelloWorld。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># helloworld</span>
sudo<span class="w"> </span>docker<span class="w"> </span>run<span class="w"> </span>hello-world
</pre></div>
</div>
<p>可以看到类似下面的画面，表示运行成功。</p>
<img width="600" alt="3" src="https://github.com/isLinXu/llm-notes/assets/59380685/7ce6ccf1-0cac-4a2e-b4bb-18718921089c">
</section>
<section id="tritonserver-api">
<h3>TritonServer推理+API服务<a class="headerlink" href="#tritonserver-api" title="Permalink to this heading">#</a></h3>
<blockquote>
<div><p>注意：这部分需要 Docker 服务。</p>
</div></blockquote>
<p>这里我们把提供模型推理服务的引擎从 TurboMind 换成了 TritonServer，启动命令就一行。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ApiServer+Triton</span>
bash<span class="w"> </span>workspace/service_docker_up.sh
</pre></div>
</div>
<p>这里会启动一个 TritonServer 的容器，如下图所示。</p>
<img width="1054" alt="14" src="https://github.com/isLinXu/llm-notes/assets/59380685/4df51594-b0e1-4e11-857d-4fb99e88dfa7">
<p>可以再开一个窗口执行 Client 命令。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># ChatTritonClient + TritonServer（注意是gRPC协议，不要用http）</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>triton_client<span class="w">  </span>localhost:33337
</pre></div>
</div>
<p>结果如下图所示。</p>
<img width="686" alt="15" src="https://github.com/isLinXu/llm-notes/assets/59380685/f4a97ad2-daa3-4d48-ae5a-52706e13cae2">
</section>
<section id="id20">
<h3>TritonServer 服务作为后端<a class="headerlink" href="#id20" title="Permalink to this heading">#</a></h3>
<p>使用过程同 2.4.1 小节。</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Gradio+TritonServer（注意是gRPC协议，不要用http）</span>
lmdeploy<span class="w"> </span>serve<span class="w"> </span>gradio<span class="w"> </span>localhost:33337<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_name<span class="w"> </span><span class="m">0</span>.0.0.0<span class="w"> </span><span class="se">\</span>
<span class="w">	</span>--server_port<span class="w"> </span><span class="m">6006</span>
</pre></div>
</div>
<p>结果如下图所示。</p>
<img width="1292" alt="18" src="https://github.com/isLinXu/llm-notes/assets/59380685/67518910-e3b6-4be2-a57a-0f136f61f1c9">
</section>
</section>
</section>


                </article>
              

              
              
                <footer class="bd-footer-article">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="4-XTuner%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%8D%95%E5%8D%A1%E4%BD%8E%E6%88%90%E6%9C%AC%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">4-XTuner大模型单卡低成本微调实战</p>
      </div>
    </a>
    <a class="right-next"
       href="6-OpenCompass%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E8%A7%A3%E8%AF%BB%E5%8F%8A%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">6-OpenCompass大模型评测解读及实战指南</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">5-LMDeploy大模型量化部署实践</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">LMDeploy 的量化和部署</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">1 环境配置</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">2 服务部署</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">2.1 模型转换</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">2.1.1 在线转换</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">2.1.2 离线转换</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind">2.2  TurboMind 推理+命令行本地对话</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind-api">2.3 TurboMind推理+API服务</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demo">2.4 网页 Demo 演示</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">2.4.1 TurboMind 服务作为后端</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">2.4.2 TurboMind 推理作为后端</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#turbomind-python">2.5 TurboMind 推理 + Python 代码集成</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">2.6 这么多，头秃，有没有最佳实践</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">2.6.1 方案实践</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">2.6.2 模型配置实践</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">3 模型量化</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kv-cache">3.1 KV Cache 量化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">3.1.1 量化步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">3.1.2 量化效果</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#w4a16">3.2 W4A16 量化</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">3.2.1 量化步骤</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">3.2.2 量化效果</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">3.3 最佳实践</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">参考资料</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tritonserver">附录1：TritonServer 作为推理引擎</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">TritonServer环境配置</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tritonserver-api">TritonServer推理+API服务</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">TritonServer 服务作为后端</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            <div class="bd-footer-content__inner">
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By isLinXu
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024, isLinXu.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div></div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=12da95d707ffb74b382d"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=12da95d707ffb74b382d"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>